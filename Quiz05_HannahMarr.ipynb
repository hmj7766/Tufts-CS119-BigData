{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3cfc9-5aa7-4c61-8ead-baf9e296144a",
   "metadata": {},
   "source": [
    "# Quiz 05: Spark APIs [100 points]\n",
    "\n",
    "## Author: Hannah Marr\n",
    "\n",
    "## CS 119"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1053fa-5988-4f05-b159-9651b4bf81f9",
   "metadata": {},
   "source": [
    "## Accumulators [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e625b-f9ce-4b5f-be71-da15fa7001d9",
   "metadata": {},
   "source": [
    "1. [10 points].The title of this Q&A is wrong. It’s really about global variables (aka accumulators). The question shows code that is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310b13c8-009a-4b8e-ba7f-c72d0069ab96",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1178760134.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    val data = Array(1,2,3,4,5)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "val data = Array(1,2,3,4,5)\n",
    "var counter = 0\n",
    "var rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "rdd.foreach(x => counter += x)\n",
    "\n",
    "println(\"Counter value: \" + counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4ec43-1dd6-4185-a313-9f00762766fa",
   "metadata": {},
   "source": [
    "Write a corrected version of the code and demonstrate its intended operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1460148-ec0c-41ff-af9f-2bdbf8496c4e",
   "metadata": {},
   "source": [
    "The issue with the original code is that Spark’s transformations and actions, such as foreach, are executed in parallel across multiple worker nodes. Since counter is a global variable, it is not properly synchronized across these nodes, leading to inconsistent results. Modifying global variables inside a distributed action like foreach is not recommended because each node has its own copy of the variable.\n",
    "\n",
    "Instead, you should use Accumulators in Spark, which are designed for safe updates across multiple worker nodes. The following is a corrected version of the code using an accumulator to achieve the intended operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05d451f3-54d0-4c73-82ef-2cbf9ed248f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1628007579.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    val data = Array(1, 2, 3, 4, 5)\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Corrected PySpark code using Accumulators (will not run in Jupyter)\n",
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val counter = sc.longAccumulator(\"Counter Accumulator\")\n",
    "val rdd = sc.parallelize(data)\n",
    "\n",
    "rdd.foreach(x => counter.add(x))\n",
    "\n",
    "println(\"Counter value: \" + counter.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241022b4-3ee0-4571-8181-daba02bb1c11",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "Accumulator: This is a special variable that allows safe and distributed accumulation of values across different nodes. Here we use a longAccumulator, which is a long-type accumulator initialized to zero.\n",
    "\n",
    "rdd.foreach: Instead of updating the global variable counter, we now add the values to the accumulator using counter.add(x).\n",
    "\n",
    "counter.value: After the action completes, we retrieve the accumulated value using counter.value.\n",
    "\n",
    "Intended Operation:\n",
    "\n",
    "The RDD data is parallelized across different worker nodes.\n",
    "\n",
    "Each worker processes part of the data and adds to the shared accumulator.\n",
    "\n",
    "After all the workers finish processing, the final value of the accumulator (sum of all elements in the array) is printed.\n",
    "\n",
    "For the array [1, 2, 3, 4, 5], the output would be: Counter value: 15\n",
    "\n",
    "This ensures that the code runs correctly in parallel while safely aggregating the results across all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a7ae2a-3611-4737-b588-22402d6a64c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/anaconda3/lib/python3.12/site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/17 10:10:25 WARN Utils: Your hostname, Hannahs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.243.30.11 instead (on interface en0)\n",
      "24/10/17 10:10:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/17 10:10:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter value:  15\n"
     ]
    }
   ],
   "source": [
    "# Code implemented in a Python environment\n",
    "!pip install pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Accumulator Example\")\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Initialize an accumulator with initial value 0\n",
    "counter = sc.accumulator(0)\n",
    "\n",
    "# Use foreach to add each element to the accumulator\n",
    "rdd.foreach(lambda x: counter.add(x))\n",
    "\n",
    "# Print the accumulated value\n",
    "print(\"Counter value: \", counter.value)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa5404-2a4e-4295-924d-b0bf4e9f88a5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97051c6-d73b-448e-a650-5a560170aab4",
   "metadata": {},
   "source": [
    "## Airline Traffic [45 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5a170-a5c2-46a5-bbd2-9b4ae505719e",
   "metadata": {},
   "source": [
    "Ontime statistics for domestic airlines are published by the Bureau of Transportation Statistics. The schema is here, but the actual data has 4 additional columns (between B. and C.) which are not documented and may be safely deleted for the purpose of this exercise.\n",
    "Based on the statistics for June 2024 and July 2024, please report on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c0122-3c0c-4a4d-9797-f56d79762bb0",
   "metadata": {},
   "source": [
    "1. [15 points] Describe in words and in code (where applicable) the steps you took to set up the environment for gathering the statistical data in the below questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57cd2b-3914-4188-b3b9-5e805456fa0c",
   "metadata": {},
   "source": [
    "Step 1: Unzipping the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd9f4472-1043-44b5-ad72-827071001f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ontime.td.202407.asc', 'ontime.td.202406.asc'],\n",
       " ['/Users/hannahmarr/Desktop/Tufts/CS119/Quizzes/ONTIME.TD.202407.REL01.03SEP2024',\n",
       "  '/Users/hannahmarr/Desktop/Tufts/CS119/Quizzes/ONTIME.TD.202406.REL01.06AUG2024'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the paths to the uploaded zip files\n",
    "zip_files = [\n",
    "    '/Users/hannahmarr/Desktop/Tufts/CS119/Quizzes/ONTIME.TD.202407.REL01.03SEP2024.zip',\n",
    "    '/Users/hannahmarr/Desktop/Tufts/CS119/Quizzes/ONTIME.TD.202406.REL01.06AUG2024.zip'\n",
    "]\n",
    "\n",
    "# Extract the contents of the zip files\n",
    "extracted_paths = []\n",
    "for zip_file in zip_files:\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        extract_path = zip_file.replace('.zip', '')  # Extract to a folder with the same name\n",
    "        zip_ref.extractall(extract_path)\n",
    "        extracted_paths.append(extract_path)\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files = []\n",
    "for path in extracted_paths:\n",
    "    extracted_files.extend(os.listdir(path))\n",
    "\n",
    "extracted_files, extracted_paths  # Display the extracted files and directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53960292-4e5c-4dcd-95bb-4b5160a16fc8",
   "metadata": {},
   "source": [
    "The ZIP files have been successfully extracted, and the contents are as follows:\n",
    "\n",
    "ontime.td.202407.asc (for July 2024)\n",
    "\n",
    "ontime.td.202406.asc (for June 2024)\n",
    "\n",
    "Both files are in .asc format, which typically means they are text files with a structured format (likely tab-delimited or fixed-width columns). Next, I will load these .asc files into pandas for inspection and proceed with the data analysis. ​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bac4b30e-23f8-4729-bcf1-452261900575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['DL|4800|||9E|4800|CHS|JFK|20240607|5|700|700|650|900|900|841|0|0|120|111|-10|-19|-9|705|830|N272PQ|15|11|85||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240608|6|700|700|654|900|900|849|0|0|120|115|-6|-11|-5|708|841|N302PQ|14|8|93||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240609|7|700|700|656|900|900|848|0|0|120|112|-4|-12|-8|710|840|N676CA|14|8|90||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240610|1|700|700|1043|900|900|1220|0|0|120|97|223|200|-23|1056|1214|N301PQ|13|6|78||4|0|0|0|196|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240611|2|700|700|657|900|900|847|0|0|120|110|-3|-13|-10|715|840|N335PQ|18|7|85||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240612|3|700|700|658|900|900|842|0|0|120|104|-2|-18|-16|713|836|N932XJ|15|6|83||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240613|4|700|700|1817|900|900|2106|0|0|120|169|677|726|49|1917|2045|N691CA|60|21|88||677|0|49|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240614|5|700|700|659|900|900|848|0|0|120|109|-1|-12|-11|711|840|N186PQ|12|8|89||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240615|6|700|700|659|900|900|852|0|0|120|113|-1|-8|-7|714|841|N604LR|15|11|87||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240616|7|700|700|658|900|900|844|0|0|120|106|-2|-16|-14|707|837|N316PQ|9|7|90||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n'],\n",
       " ['DL|4800|||9E|4800|CHS|JFK|20240701|1|700|700|656|900|900|900|0|0|120|124|-4|0|4|710|841|N935XJ|14|19|91||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240702|2|700|700|659|900|900|905|0|0|120|126|-1|5|6|719|855|N335PQ|20|10|96||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240703|3|700|700|655|900|900|852|0|0|120|117|-5|-8|-3|708|842|N337PQ|13|10|94||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240704|4|700|700|655|900|900|853|0|0|120|118|-5|-7|-2|707|847|N272PQ|12|6|100||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240705|5|700|700|656|900|900|850|0|0|120|114|-4|-10|-6|710|840|N146PQ|14|10|90||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240706|6|700|700|654|900|900|846|0|0|120|112|-6|-14|-8|706|838|N310PQ|12|8|92||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|CHS|JFK|20240707|7|700|700|657|900|900|840|0|0|120|103|-3|-20|-17|710|836|N926XJ|13|4|86||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|HPN|ATL|20240708|1|1500|1500|1456|1725|1725|1702|0|0|145|126|-4|-23|-19|1506|1656|N368CA|10|6|110||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|HPN|ATL|20240709|2|1500|1500|1633|1725|1725|1922|0|0|145|169|93|117|24|1711|1910|N340CA|38|12|119||90|0|24|0|3|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n',\n",
       "  'DL|4800|||9E|4800|HPN|ATL|20240710|3|1500|1500|1451|1725|1725|1710|0|0|145|139|-9|-15|-6|1514|1704|N303PQ|23|6|110||0|0|0|0|0|0|0|0|0||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0|||0|0|0|0||FORM-1|N\\n'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .asc files to inspect the format and structure\n",
    "june_file = '/Users/hannahmarr/Desktop/Tufts/CS119/Quizzes/ONTIME.TD.202406.REL01.06AUG2024/ontime.td.202406.asc'\n",
    "july_file = '/Users/hannahmarr/Desktop/Tufts/CS119/Quizzes/ONTIME.TD.202407.REL01.03SEP2024/ontime.td.202407.asc'\n",
    "\n",
    "# Read a few lines from each file to inspect the format\n",
    "with open(june_file, 'r') as june_f, open(july_file, 'r') as july_f:\n",
    "    june_preview = [next(june_f) for _ in range(10)]\n",
    "    july_preview = [next(july_f) for _ in range(10)]\n",
    "\n",
    "june_preview, july_preview  # Display the first 10 lines of both files for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af97d7c5-2bea-4d18-a6c3-c108d37d82d7",
   "metadata": {},
   "source": [
    "The .asc files are delimited by vertical bars (|), which suggests that they can be treated as delimited text files for easier loading into a DataFrame. The structure contains information about flights, including airline codes, airports, departure times, delays, etc.\n",
    "\n",
    "Next, I will load these files into pandas DataFrames, clean the data by removing the unnecessary columns, and start analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c3d18c85-bcef-476e-a76e-aade4d9d202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/m5frxr_n1c19zhr2wxwwrclh0000gn/T/ipykernel_70348/3798967454.py:20: DtypeWarning: Columns (2,45,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  june_data = pd.read_csv(june_file, sep='|', names = column_names)\n",
      "/var/folders/1p/m5frxr_n1c19zhr2wxwwrclh0000gn/T/ipykernel_70348/3798967454.py:21: DtypeWarning: Columns (2,29,45,50) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  july_data = pd.read_csv(july_file, sep='|', names = column_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 84\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carrier</th>\n",
       "      <th>Flight_Number</th>\n",
       "      <th>Departure_Airport</th>\n",
       "      <th>Arrival_Airport</th>\n",
       "      <th>Date_of_Flight_Operation_YMD</th>\n",
       "      <th>Day_of_Week_of_Flight_Operation_M=1</th>\n",
       "      <th>Scheduled_Departure_Time1</th>\n",
       "      <th>Scheduled_Departure_Time2</th>\n",
       "      <th>Gate_Departure_Time</th>\n",
       "      <th>Scheduled_Arrival_Time_OAG</th>\n",
       "      <th>...</th>\n",
       "      <th>Elapsed_Time_Diff</th>\n",
       "      <th>Wheels-Off_Time</th>\n",
       "      <th>Wheels-On_Time</th>\n",
       "      <th>Aircraft_Tail_Number</th>\n",
       "      <th>Cancellation_Code</th>\n",
       "      <th>Mins_Late_E</th>\n",
       "      <th>Mins_Late_F</th>\n",
       "      <th>Mins_Late_G</th>\n",
       "      <th>Mins_Late_H</th>\n",
       "      <th>Mins_Late_I</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240607</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>650</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>-9</td>\n",
       "      <td>705</td>\n",
       "      <td>830</td>\n",
       "      <td>N272PQ</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240608</td>\n",
       "      <td>6</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>654</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>-5</td>\n",
       "      <td>708</td>\n",
       "      <td>841</td>\n",
       "      <td>N302PQ</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240609</td>\n",
       "      <td>7</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>656</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>-8</td>\n",
       "      <td>710</td>\n",
       "      <td>840</td>\n",
       "      <td>N676CA</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240610</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>1043</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>-23</td>\n",
       "      <td>1056</td>\n",
       "      <td>1214</td>\n",
       "      <td>N301PQ</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240611</td>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>657</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>-10</td>\n",
       "      <td>715</td>\n",
       "      <td>840</td>\n",
       "      <td>N335PQ</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240612</td>\n",
       "      <td>3</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>658</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>-16</td>\n",
       "      <td>713</td>\n",
       "      <td>836</td>\n",
       "      <td>N932XJ</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240613</td>\n",
       "      <td>4</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>1817</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>1917</td>\n",
       "      <td>2045</td>\n",
       "      <td>N691CA</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240614</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>659</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>-11</td>\n",
       "      <td>711</td>\n",
       "      <td>840</td>\n",
       "      <td>N186PQ</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240615</td>\n",
       "      <td>6</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>659</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>-7</td>\n",
       "      <td>714</td>\n",
       "      <td>841</td>\n",
       "      <td>N604LR</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DL</td>\n",
       "      <td>4800</td>\n",
       "      <td>CHS</td>\n",
       "      <td>JFK</td>\n",
       "      <td>20240616</td>\n",
       "      <td>7</td>\n",
       "      <td>700</td>\n",
       "      <td>700</td>\n",
       "      <td>658</td>\n",
       "      <td>900</td>\n",
       "      <td>...</td>\n",
       "      <td>-14</td>\n",
       "      <td>707</td>\n",
       "      <td>837</td>\n",
       "      <td>N316PQ</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Carrier  Flight_Number Departure_Airport Arrival_Airport  \\\n",
       "0      DL           4800               CHS             JFK   \n",
       "1      DL           4800               CHS             JFK   \n",
       "2      DL           4800               CHS             JFK   \n",
       "3      DL           4800               CHS             JFK   \n",
       "4      DL           4800               CHS             JFK   \n",
       "5      DL           4800               CHS             JFK   \n",
       "6      DL           4800               CHS             JFK   \n",
       "7      DL           4800               CHS             JFK   \n",
       "8      DL           4800               CHS             JFK   \n",
       "9      DL           4800               CHS             JFK   \n",
       "\n",
       "   Date_of_Flight_Operation_YMD  Day_of_Week_of_Flight_Operation_M=1  \\\n",
       "0                      20240607                                    5   \n",
       "1                      20240608                                    6   \n",
       "2                      20240609                                    7   \n",
       "3                      20240610                                    1   \n",
       "4                      20240611                                    2   \n",
       "5                      20240612                                    3   \n",
       "6                      20240613                                    4   \n",
       "7                      20240614                                    5   \n",
       "8                      20240615                                    6   \n",
       "9                      20240616                                    7   \n",
       "\n",
       "   Scheduled_Departure_Time1  Scheduled_Departure_Time2  Gate_Departure_Time  \\\n",
       "0                        700                        700                  650   \n",
       "1                        700                        700                  654   \n",
       "2                        700                        700                  656   \n",
       "3                        700                        700                 1043   \n",
       "4                        700                        700                  657   \n",
       "5                        700                        700                  658   \n",
       "6                        700                        700                 1817   \n",
       "7                        700                        700                  659   \n",
       "8                        700                        700                  659   \n",
       "9                        700                        700                  658   \n",
       "\n",
       "   Scheduled_Arrival_Time_OAG  ...  Elapsed_Time_Diff  Wheels-Off_Time  \\\n",
       "0                         900  ...                 -9              705   \n",
       "1                         900  ...                 -5              708   \n",
       "2                         900  ...                 -8              710   \n",
       "3                         900  ...                -23             1056   \n",
       "4                         900  ...                -10              715   \n",
       "5                         900  ...                -16              713   \n",
       "6                         900  ...                 49             1917   \n",
       "7                         900  ...                -11              711   \n",
       "8                         900  ...                 -7              714   \n",
       "9                         900  ...                -14              707   \n",
       "\n",
       "   Wheels-On_Time  Aircraft_Tail_Number  Cancellation_Code  Mins_Late_E  \\\n",
       "0             830                N272PQ                 15           11   \n",
       "1             841                N302PQ                 14            8   \n",
       "2             840                N676CA                 14            8   \n",
       "3            1214                N301PQ                 13            6   \n",
       "4             840                N335PQ                 18            7   \n",
       "5             836                N932XJ                 15            6   \n",
       "6            2045                N691CA                 60           21   \n",
       "7             840                N186PQ                 12            8   \n",
       "8             841                N604LR                 15           11   \n",
       "9             837                N316PQ                  9            7   \n",
       "\n",
       "   Mins_Late_F  Mins_Late_G  Mins_Late_H  Mins_Late_I  \n",
       "0           85          NaN            0            0  \n",
       "1           93          NaN            0            0  \n",
       "2           90          NaN            0            0  \n",
       "3           78          NaN            4            0  \n",
       "4           85          NaN            0            0  \n",
       "5           83          NaN            0            0  \n",
       "6           88          NaN          677            0  \n",
       "7           89          NaN            0            0  \n",
       "8           87          NaN            0            0  \n",
       "9           90          NaN            0            0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the .asc files into pandas DataFrames\n",
    "column_names = [\n",
    "    'Carrier', 'Flight_Number', 'UNDOC_1', 'UNDOC_2', 'UNDOC_3', 'UNDOC_4', 'Departure_Airport', 'Arrival_Airport',\n",
    "    'Date_of_Flight_Operation_YMD', 'Day_of_Week_of_Flight_Operation_M=1', 'Scheduled_Departure_Time1', 'Scheduled_Departure_Time2',\n",
    "    'Gate_Departure_Time', 'Scheduled_Arrival_Time_OAG', 'Scheduled_Arrival_Time_CRS', 'Gate_Arrival_Time', \n",
    "    'Min_Diff_OAG_Scheduled_Depart_Time', 'Min_Diff_OAG_Scheduled_Arrive_Time', 'Elapsed_Time_CRS_Mins', 'Gate_to_Gate_Time_Actual', \n",
    "    'Departure_Delay', 'Arrival_Delay', 'Elapsed_Time_Diff', 'Wheels-Off_Time', 'Wheels-On_Time', 'Aircraft_Tail_Number', \n",
    "    'Cancellation_Code', 'Mins_Late_E', 'Mins_Late_F', 'Mins_Late_G', 'Mins_Late_H', 'Mins_Late_I', 'UNDOC_33', 'UNDOC_34', 'UNDOC_35', \n",
    "    'UNDOC_36', 'UNDOC_37', 'UNDOC_38', 'UNDOC_39', 'UNDOC_40', 'UNDOC_41', 'UNDOC_42', 'UNDOC_43', 'UNDOC_44', 'UNDOC_45', 'UNDOC_46', \n",
    "    'UNDOC_47', 'UNDOC_48', 'UNDOC_49', 'UNDOC_50', 'UNDOC_51', 'UNDOC_52', 'UNDOC_53', 'UNDOC_54', 'UNDOC_55', 'UNDOC_56', 'UNDOC_57', \n",
    "    'UNDOC_58', 'UNDOC_59', 'UNDOC_60', 'UNDOC_61', 'UNDOC_62', 'UNDOC_63', 'UNDOC_64', 'UNDOC_65', 'UNDOC_66', 'UNDOC_67', 'UNDOC_68', \n",
    "    'UNDOC_69', 'UNDOC_70', 'UNDOC_71', 'UNDOC_72', 'UNDOC_73', 'UNDOC_74', 'UNDOC_75', 'UNDOC_76', 'UNDOC_77', 'UNDOC_78', 'UNDOC_79', \n",
    "    'UNDOC_80', 'UNDOC_81', 'UNDOC_82', 'UNDOC_83', 'UNDOC_84'\n",
    "]\n",
    "\n",
    "# Load June and July data\n",
    "june_data = pd.read_csv(june_file, sep='|', names = column_names)\n",
    "july_data = pd.read_csv(july_file, sep='|', names = column_names)\n",
    "\n",
    "# Combine both months into one dataframe\n",
    "all_data = pd.concat([june_data, july_data], ignore_index=True)\n",
    "\n",
    "# Check the number of columns (this was to determine the number of undocumented columns that I would need to drop)\n",
    "print(\"Number of columns:\", all_data.shape[1])\n",
    "\n",
    "# Drop the undocumented columns\n",
    "all_data.drop(columns=['UNDOC_1', 'UNDOC_2', 'UNDOC_3', 'UNDOC_4', 'UNDOC_33', 'UNDOC_34', 'UNDOC_35', \n",
    "    'UNDOC_36', 'UNDOC_37', 'UNDOC_38', 'UNDOC_39', 'UNDOC_40', 'UNDOC_41', 'UNDOC_42', 'UNDOC_43', 'UNDOC_44', 'UNDOC_45', 'UNDOC_46', \n",
    "    'UNDOC_47', 'UNDOC_48', 'UNDOC_49', 'UNDOC_50', 'UNDOC_51', 'UNDOC_52', 'UNDOC_53', 'UNDOC_54', 'UNDOC_55', 'UNDOC_56', 'UNDOC_57', \n",
    "    'UNDOC_58', 'UNDOC_59', 'UNDOC_60', 'UNDOC_61', 'UNDOC_62', 'UNDOC_63', 'UNDOC_64', 'UNDOC_65', 'UNDOC_66', 'UNDOC_67', 'UNDOC_68', \n",
    "    'UNDOC_69', 'UNDOC_70', 'UNDOC_71', 'UNDOC_72', 'UNDOC_73', 'UNDOC_74', 'UNDOC_75', 'UNDOC_76', 'UNDOC_77', 'UNDOC_78', 'UNDOC_79', \n",
    "    'UNDOC_80', 'UNDOC_81', 'UNDOC_82', 'UNDOC_83', 'UNDOC_84'], inplace=True)\n",
    "\n",
    "# Display the first few rows of the combined dataset\n",
    "all_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952e82c-42ff-4f15-8786-75ac5e65bb95",
   "metadata": {},
   "source": [
    "2. [6 points] Which US Airline Has the Least Delays? Report by full names, (e.g., Delta Airlines, not DL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a8e28705-4f73-4b40-b742-284cb107c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHS' 'ATL' 'FSD' 'MSP' 'TRI' 'ABE' 'TYS' 'LGA' 'JFK' 'MCI' 'DTW' 'CLT'\n",
      " 'RIC' 'ROC' 'CHO' 'IND' 'MQT' 'PWM' 'CVG' 'TVC' 'DSM' 'PIT' 'CHA' 'ORF'\n",
      " 'ILM' 'CLE' 'ORD' 'CSG' 'CAE' 'GSP' 'BUF' 'MEM' 'RDU' 'STL' 'CWA' 'OMA'\n",
      " 'SAV' 'PNS' 'GNV' 'XNA' 'BHM' 'MKE' 'JAX' 'GTR' 'EWR' 'BNA' 'MLI' 'MLU'\n",
      " 'LIT' 'MSN' 'BDL' 'GSO' 'MGM' 'HPN' 'DLH' 'SDF' 'ALB' 'AVL' 'GRR' 'AEX'\n",
      " 'DAY' 'MYR' 'BGR' 'SYR' 'PVD' 'BTV' 'HSV' 'RAP' 'ROA' 'FAY' 'SHV' 'TUL'\n",
      " 'ORH' 'CMH' 'EVV' 'BTR' 'DCA' 'LFT' 'BGM' 'MOB' 'TLH' 'ATW' 'MDT' 'OAJ'\n",
      " 'VLD' 'ITH' 'RST' 'AGS' 'DHN' 'BWI' 'AUS' 'MBS' 'BMI' 'BQK' 'FAR' 'ABY'\n",
      " 'GRB' 'SFO' 'DFW' 'SRQ' 'LAX' 'PHX' 'SJC' 'SNA' 'MIA' 'PHL' 'SAT' 'STT'\n",
      " 'MCO' 'SMF' 'MSO' 'SEA' 'FAT' 'BZN' 'TUS' 'MSY' 'OKC' 'BOS' 'SJU' 'ELP'\n",
      " 'TPA' 'LAS' 'FLL' 'ABQ' 'PDX' 'BFL' 'DEN' 'AVP' 'SAN' 'IAH' 'RSW' 'SBA'\n",
      " 'PBI' 'ONT' 'JAC' 'CID' 'DRO' 'ECP' 'VPS' 'RNO' 'EYW' 'PSP' 'FCA' 'SLC'\n",
      " 'SBP' 'BUR' 'MHT' 'DAB' 'IAD' 'GEG' 'LEX' 'MFE' 'RDM' 'OGG' 'KOA' 'MTJ'\n",
      " 'HNL' 'LIH' 'STS' 'ICT' 'MRY' 'STX' 'LBB' 'COS' 'BOI' 'EGE' 'ANC' 'EUG'\n",
      " 'BET' 'BRW' 'SCC' 'SIT' 'JNU' 'KTN' 'YAK' 'CDV' 'WRG' 'PSG' 'GST' 'ADQ'\n",
      " 'FAI' 'AKN' 'OTZ' 'OME' 'ADK' 'PAE' 'PSC' 'OAK' 'DAL' 'DLG' 'ACK' 'BQN'\n",
      " 'PSE' 'MVY' 'HYA' 'LRD' 'MAF' 'GPT' 'HRL' 'CRP' 'SGF' 'JAN' 'BRO' 'LCH'\n",
      " 'SCE' 'AMA' 'SAF' 'MDW' 'MLB' 'BIS' 'BIL' 'HOU' 'LGB' 'ISP' 'TTN' 'AZA'\n",
      " 'FWA' 'SFB' 'PIE' 'FNT' 'USA' 'PVU' 'RFD' 'BLV' 'PGD' 'ELM' 'LCK' 'MFR'\n",
      " 'HTS' 'HGR' 'SWF' 'PBG' 'BLI' 'PIA' 'SBN' 'PSM' 'CAK' 'MOT' 'GTF' 'GJT'\n",
      " 'IDA' 'TOL' 'CKB' 'GRI' 'SCK' 'IAG' 'SPI' 'GFK' 'SMX' 'STC' 'PQI' 'ITO'\n",
      " 'PPG' 'ACT' 'TXK' 'GCK' 'ABI' 'GGG' 'GRK' 'MHK' 'CMI' 'COU' 'FSM' 'HHH'\n",
      " 'SJT' 'ROW' 'LSE' 'CLL' 'YUM' 'ACY' 'LBE' 'CRW' 'LAN' 'ASE' 'XWA' 'AZO'\n",
      " 'HIB' 'SGU' 'CIU' 'ESC' 'EKO' 'TYR' 'SWO' 'LAW' 'BPT' 'FLG' 'PLN' 'LBF'\n",
      " 'COD' 'MCW' 'JST' 'DVL' 'PIB' 'LNK' 'PRC' 'CPR' 'BFF' 'DDC' 'LBL' 'SUX'\n",
      " 'CYS' 'VCT' 'JMS' 'HYS' 'FOD' 'MEI' 'RKS' 'JLN' 'HOB' 'RDD' 'RHI' 'TWF'\n",
      " 'ABR' 'ACV' 'GUC' 'LWS' 'OTH' 'SUN' 'HDN' 'HLN' 'INL' 'BTM' 'WYS' 'SPS'\n",
      " 'APN' 'BRD' 'CDC' 'BJI' 'DEC' 'RIW' 'SHR' 'LAR' 'CMX' 'SLN' 'IMT' 'DIK'\n",
      " 'GCC' 'PIH' 'ALW' 'LYH' 'ART' 'EWN' 'SBY' 'PHF' 'FLO' 'PGV' 'ERI' 'YKM'\n",
      " 'PUW' 'EAT' 'GUM' 'SPN' 'ALO' 'BIH']\n"
     ]
    }
   ],
   "source": [
    "# Extract unique airline carrier codes from Carrier column\n",
    "all_airline_carriers = all_data['Departure_Airport'].unique()\n",
    "\n",
    "# Display unique airport codes\n",
    "print(all_airline_carriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "da05fd65-f7bf-4594-a809-66ae6d6a089c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airline with the least delays: HA\n",
      "Departure_Delay    5.805322\n",
      "Arrival_Delay      4.851308\n",
      "Mean_Delay         5.328315\n",
      "Name: HA, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean departure and arrival delays for each airline\n",
    "airline_delay = all_data.groupby('Carrier')[['Departure_Delay', 'Arrival_Delay']].mean()\n",
    "airline_delay['Mean_Delay'] = airline_delay.mean(axis=1)  # Average of departure and arrival delays\n",
    "\n",
    "# Identify the airline with the least delays\n",
    "least_delayed_airline = airline_delay.sort_values('Mean_Delay').iloc[0]\n",
    "\n",
    "# Retrieve the airline carrier code from the index\n",
    "carrier_of_least_delayed_airline = airline_delay.sort_values('Mean_Delay').index[0]\n",
    "\n",
    "# Display the result\n",
    "print(\"Airline with the least delays:\", carrier_of_least_delayed_airline)\n",
    "print(least_delayed_airline)  # Display the full row for details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7832029-5a1e-4e46-8f2f-8f6c7f0199a3",
   "metadata": {},
   "source": [
    "The airline with the least delays is Hawaiian Airlines (carrier code HA), with an average delay time of 5.33 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964f40f-3d0a-4e1c-b0a4-83624e32b860",
   "metadata": {},
   "source": [
    "3. [6 points] What Departure Time of Day Is Best to Avoid Flight Delays, segmented into 5 time blocks [night (10 pm - 6 am), morning (6 am to 10 am), mid-day (10 am to 2 pm), afternoon (2 pm - 6 pm), evening (6 pm - 10 pm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dcbc6b9f-6c68-4959-9a66-77b35256c81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid time entries: Empty DataFrame\n",
      "Columns: [Carrier, Flight_Number, Departure_Airport, Arrival_Airport, Date_of_Flight_Operation_YMD, Day_of_Week_of_Flight_Operation_M=1, Scheduled_Departure_Time1, Scheduled_Departure_Time2, Gate_Departure_Time, Scheduled_Arrival_Time_OAG, Scheduled_Arrival_Time_CRS, Gate_Arrival_Time, Min_Diff_OAG_Scheduled_Depart_Time, Min_Diff_OAG_Scheduled_Arrive_Time, Elapsed_Time_CRS_Mins, Gate_to_Gate_Time_Actual, Departure_Delay, Arrival_Delay, Elapsed_Time_Diff, Wheels-Off_Time, Wheels-On_Time, Aircraft_Tail_Number, Cancellation_Code, Mins_Late_E, Mins_Late_F, Mins_Late_G, Mins_Late_H, Mins_Late_I, Formatted_Gate_Departure_Time]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 29 columns]\n",
      "Time_Block\n",
      "Afternoon     20.004364\n",
      "Evening       31.640302\n",
      "Mid-day       13.862641\n",
      "Morning        6.662696\n",
      "Night         26.876475\n",
      "Unknown      107.474552\n",
      "Name: Departure_Delay, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to convert local 24-hour time without leading zeros (e.g., 650 -> \"06:50\")\n",
    "def convert_to_time_str(time_value):\n",
    "    try:\n",
    "        time_value = str(int(time_value))  # Ensure it's a string representation of an integer\n",
    "        if len(time_value) <= 2:\n",
    "            # Time is in hours only (e.g., '5' becomes '05:00')\n",
    "            return f\"{time_value.zfill(2)}:00\"\n",
    "        else:\n",
    "            # Split the last two digits as minutes, the rest as hours (e.g., '650' becomes '06:50')\n",
    "            return f\"{time_value[:-2].zfill(2)}:{time_value[-2:]}\"\n",
    "    except ValueError:\n",
    "        # If conversion fails, return NaT (Not a Time) to handle bad data\n",
    "        return pd.NaT\n",
    "\n",
    "# Apply the conversion function to the Gate_Departure_Time column\n",
    "all_data['Formatted_Gate_Departure_Time'] = all_data['Gate_Departure_Time'].apply(convert_to_time_str)\n",
    "\n",
    "# Check if any NaT or invalid values were generated\n",
    "invalid_times = all_data[all_data['Formatted_Gate_Departure_Time'].isna()]\n",
    "print(\"Invalid time entries:\", invalid_times)\n",
    "\n",
    "# Proceed to extract the hour from valid formatted times\n",
    "all_data['Hour_Gate_Departure_Time'] = pd.to_datetime(all_data['Formatted_Gate_Departure_Time'], format='%H:%M', errors='coerce').dt.hour\n",
    "\n",
    "# Define time block categories based on the extracted hour\n",
    "def time_block(hour):\n",
    "    if pd.isna(hour):\n",
    "        return 'Unknown'\n",
    "    elif 22 <= hour or hour < 6:\n",
    "        return 'Night'\n",
    "    elif 6 <= hour < 10:\n",
    "        return 'Morning'\n",
    "    elif 10 <= hour < 14:\n",
    "        return 'Mid-day'\n",
    "    elif 14 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Evening'\n",
    "\n",
    "# Apply the time block categories\n",
    "all_data['Time_Block'] = all_data['Hour_Gate_Departure_Time'].apply(time_block)\n",
    "\n",
    "# Calculate the mean delay for each time block\n",
    "time_block_delay = all_data.groupby('Time_Block')['Departure_Delay'].mean()\n",
    "\n",
    "# Print the time block delay results\n",
    "print(time_block_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccfea5-c686-4b33-a433-9df1c20752a0",
   "metadata": {},
   "source": [
    "The best departure time of day to avoid flight delays is Morning, with an average departure delay of only 6.66 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d21e7-053a-40c8-b6dd-f0bd3247b1ca",
   "metadata": {},
   "source": [
    "4. [5 points] Which Airports Have The Most Flight Delays? Report by full name, (e.g., “Newark Liberty International,” not “EWR,” when the airport code EWR is provided)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8d55317f-b7dd-436f-8f66-8672d1b651fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Departure_Delay</th>\n",
       "      <th>Arrival_Delay</th>\n",
       "      <th>Total_Delay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Departure_Airport</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DFW</th>\n",
       "      <td>1544728</td>\n",
       "      <td>1344845</td>\n",
       "      <td>2889573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLT</th>\n",
       "      <td>1279648</td>\n",
       "      <td>1124205</td>\n",
       "      <td>2403853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORD</th>\n",
       "      <td>1283296</td>\n",
       "      <td>1110008</td>\n",
       "      <td>2393304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATL</th>\n",
       "      <td>1213106</td>\n",
       "      <td>1043339</td>\n",
       "      <td>2256445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEN</th>\n",
       "      <td>1032185</td>\n",
       "      <td>809265</td>\n",
       "      <td>1841450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YKM</th>\n",
       "      <td>-19</td>\n",
       "      <td>-493</td>\n",
       "      <td>-512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WRG</th>\n",
       "      <td>-169</td>\n",
       "      <td>-434</td>\n",
       "      <td>-603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PIH</th>\n",
       "      <td>81</td>\n",
       "      <td>-713</td>\n",
       "      <td>-632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKN</th>\n",
       "      <td>-644</td>\n",
       "      <td>-48</td>\n",
       "      <td>-692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCC</th>\n",
       "      <td>-1017</td>\n",
       "      <td>-1156</td>\n",
       "      <td>-2173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>354 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Departure_Delay  Arrival_Delay  Total_Delay\n",
       "Departure_Airport                                             \n",
       "DFW                        1544728        1344845      2889573\n",
       "CLT                        1279648        1124205      2403853\n",
       "ORD                        1283296        1110008      2393304\n",
       "ATL                        1213106        1043339      2256445\n",
       "DEN                        1032185         809265      1841450\n",
       "...                            ...            ...          ...\n",
       "YKM                            -19           -493         -512\n",
       "WRG                           -169           -434         -603\n",
       "PIH                             81           -713         -632\n",
       "AKN                           -644            -48         -692\n",
       "SCC                          -1017          -1156        -2173\n",
       "\n",
       "[354 rows x 3 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum arrival and departure delays for each airport\n",
    "airport_delays = all_data.groupby('Departure_Airport')[['Departure_Delay', 'Arrival_Delay']].sum()\n",
    "airport_delays['Total_Delay'] = airport_delays.sum(axis=1)\n",
    "\n",
    "# Sort by total delay\n",
    "most_delayed_airports = airport_delays.sort_values('Total_Delay', ascending=False)\n",
    "most_delayed_airports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf732587-625d-4b84-a8dd-1738b2a147ff",
   "metadata": {},
   "source": [
    "The five airports with the highest flight delays, starting with the airport with the highest total departure and arrival delays, are Dallas-Fort Worth International (DFW), Charlotte - Douglas (CLT), Chicago - O'Hare (ORD), Atlanta - Hartsfield Jackson (ATL), and Denver - International (DEN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbcdd97-d8a0-4eca-9f94-bedbbdc18c16",
   "metadata": {},
   "source": [
    "5. [5 points] What Are the Top 5 Busiest Airports in the US. Report by full name, (e.g., “Newark Liberty International,” not “EWR”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "13015b79-cc20-4b76-9676-2b52ec316a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Departures  Arrivals  Total_Flights\n",
      "ATL       59567     59561         119128\n",
      "DFW       57164     57155         114319\n",
      "DEN       55986     55976         111962\n",
      "ORD       55735     55734         111469\n",
      "CLT       43837     43834          87671\n"
     ]
    }
   ],
   "source": [
    "# Count the number of flights by airport (arrivals + departures)\n",
    "busiest_airports_departures = all_data['Departure_Airport'].value_counts()\n",
    "busiest_airports_arrivals = all_data['Arrival_Airport'].value_counts()\n",
    "\n",
    "# Combine the counts of arrivals and departures by the airport code (using sum)\n",
    "busiest_airports = pd.DataFrame({\n",
    "    'Departures': busiest_airports_departures,\n",
    "    'Arrivals': busiest_airports_arrivals\n",
    "}).fillna(0)  # Fill NaN values with 0 where an airport has no departures or no arrivals\n",
    "\n",
    "# Sum both columns to get the total number of flights for each airport\n",
    "busiest_airports['Total_Flights'] = busiest_airports['Departures'] + busiest_airports['Arrivals']\n",
    "\n",
    "# Sort by the busiest airports (most flights)\n",
    "busiest_airports_sorted = busiest_airports.sort_values('Total_Flights', ascending=False)\n",
    "\n",
    "# Display the top 10 busiest airports\n",
    "print(busiest_airports_sorted.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2bc198-e984-4df4-8c42-99a070f33c61",
   "metadata": {},
   "source": [
    "The top 5 busiest airports are Atlanta - Hartsfield Jackson (ATL), Dallas-Fort Worth International (DFW), Denver - International (DEN), Chicago - O'Hare (ORD), and Charlotte - Douglas (CLT). These are also the airports with the highest flight delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3322f-f9f7-4515-8ab4-768e468defb2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51266e4d-33a8-44d6-ab9d-0a71b89e5749",
   "metadata": {},
   "source": [
    "## ShortStoryJam [45 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc4085-12b4-4714-81b2-2f94a528993c",
   "metadata": {},
   "source": [
    "ShortStoryJam is a proposed new business for users to upload their short stories. We wish to set up a framework for analyzing an arbitrarily large number of stories. We would like to be able to deploy hundreds of servers to analyze different stories in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651a5b5-3d64-4fef-b5d7-2f1ae96b661b",
   "metadata": {},
   "source": [
    "1. [3 points] To seed the effort, the text of about 22 short stories by Edgar Allan Poe, he of the “quoth the raven” fame, are available in my github repository. Clean the text and remove stopwords,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "07f60d49-91d3-4e72-9606-a7cdc842fcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:24: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:24: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/1p/m5frxr_n1c19zhr2wxwwrclh0000gn/T/ipykernel_70348/3489872935.py:20: SyntaxWarning: invalid escape sequence '\\['\n",
      "  text = re.sub('\\[.*?\\]', '', text)\n",
      "/var/folders/1p/m5frxr_n1c19zhr2wxwwrclh0000gn/T/ipykernel_70348/3489872935.py:24: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  text = re.sub('[\\d\\n]', ' ', text)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Fetch the stopwords list from the given URL\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = list(set(stopwords_list.decode().splitlines()))\n",
    "\n",
    "# Function to remove stopwords from a list of words\n",
    "def remove_stopwords(words):\n",
    "    # Clean and split the input words\n",
    "    list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split()\n",
    "    return [itm for itm in list_ if itm not in stopwords]\n",
    "\n",
    "# Function to clean the text (lowercase, remove punctuation, digits, and stopwords)\n",
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove content inside square brackets\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    # Remove digits and newlines\n",
    "    text = re.sub('[\\d\\n]', ' ', text)\n",
    "    # Remove stopwords\n",
    "    return ' '.join(remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0fa7babf-0c3f-4502-a0de-db480e5d20df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ways god nature providence ways models frame commensurate vastness profundity unsearchableness works depth greater democritus joseph glanville reached summit loftiest crag minutes man exhausted speak long ago length guided route youngest sons years happened event happened mortal man man survived hours deadly terror endured broken body soul suppose man single day change hairs jetty black white weaken limbs unstring nerves tremble exertion frightened shadow scarcely cliff giddy cliff edge careless\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Save the cleaned text back to a file\n",
    "def save_cleaned_text(file_path, cleaned_text):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cleaned_text)\n",
    "\n",
    "# Example usage with A Descent into the Maelstrom story:\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path of the downloaded text file\n",
    "    file_path = '/Users/hannahmarr/Downloads/A_DESCENT_INTO_THE_MAELSTROM.txt'\n",
    "    cleaned_file_path = '/Users/hannahmarr/Downloads/A_DESCENT_INTO_THE_MAELSTROM_CLEANED.txt'  # Path where cleaned file will be saved\n",
    "\n",
    "    # Read the original text\n",
    "    original_text = read_file(file_path)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(original_text)\n",
    "\n",
    "     # Step 5: Save the cleaned text (optional)\n",
    "    save_cleaned_text(cleaned_file_path, cleaned_text)\n",
    "\n",
    "    # Output the first 500 characters of cleaned text to verify\n",
    "    print(cleaned_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3e976881-d25c-4088-bba6-c36fc6af3b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "injuries fortunato borne ventured insult vowed revenge nature soul suppose utterance threat length avenged point definitively settled definitiveness resolved precluded idea risk punish punish impunity wrong unredressed retribution overtakes redresser equally unredressed avenger fails felt wrong understood word deed fortunato doubt good continued smile face perceive smile thought immolation weak point fortunato man respected feared prided connoisseurship wine italians true virtuoso spirit enthusi\n"
     ]
    }
   ],
   "source": [
    "# Example usage with The Cask of Amontillado story:\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path of the downloaded text file\n",
    "    file_path = '/Users/hannahmarr/Downloads/THE_CASK_OF_AMONTILLADO.txt'\n",
    "    cleaned_file_path = '/Users/hannahmarr/Downloads/THE_CASK_OF_AMONTILLADO_CLEANED.txt'  # Path where cleaned file will be saved\n",
    "\n",
    "    # Read the original text\n",
    "    original_text = read_file(file_path)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(original_text)\n",
    "\n",
    "    # Output the first 500 characters of cleaned text to verify\n",
    "    print(cleaned_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8edfff-7a26-456c-9b7e-30cc49879aaf",
   "metadata": {},
   "source": [
    "2. [8 points] Use NLTK to decompose the first story (A_DESCENT_INTO…) into sentences & sentences into tokens. Here is the code for doing that, after you set the variable paragraph to hold the text of the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "73160aad-fa56-4884-8e9a-fd6198ca41f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hannahmarr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hannahmarr/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK data (punkt for tokenization and averaged_perceptron_tagger for POS tagging)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to read the story text from a local file\n",
    "def read_story_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Decompose the story into sentences and then tokenize each sentence\n",
    "def decompose_story(paragraph):\n",
    "    # Split text into sentences\n",
    "    sent_text = nltk.sent_tokenize(paragraph)\n",
    "    # Tokenize each sentence and apply POS tagging\n",
    "    all_tagged = [nltk.pos_tag(nltk.word_tokenize(sent)) for sent in sent_text]\n",
    "    return all_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "66253cdd-6221-40d1-8c35-1c7084577ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('ways', 'NNS'), ('of', 'IN'), ('God', 'NNP'), ('in', 'IN'), ('Nature', 'NNP'), (',', ','), ('as', 'IN'), ('in', 'IN'), ('Providence', 'NNP'), (',', ','), ('are', 'VBP'), ('not', 'RB'), ('as', 'IN'), ('our', 'PRP$'), ('ways', 'NNS'), (';', ':'), ('nor', 'CC'), ('are', 'VBP'), ('the', 'DT'), ('models', 'NNS'), ('that', 'IN'), ('we', 'PRP'), ('frame', 'VBP'), ('any', 'DT'), ('way', 'NN'), ('commensurate', 'NN'), ('to', 'TO'), ('the', 'DT'), ('vastness', 'NN'), (',', ','), ('profundity', 'NN'), (',', ','), ('and', 'CC'), ('unsearchableness', 'NN'), ('of', 'IN'), ('His', 'PRP$'), ('works', 'NNS'), (',', ','), ('_which', 'NNS'), ('have', 'VBP'), ('a', 'DT'), ('depth', 'NN'), ('in', 'IN'), ('them', 'PRP'), ('greater', 'JJR'), ('than', 'IN'), ('the', 'DT'), ('well', 'NN'), ('of', 'IN'), ('Democritus_', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Example usage with A Descent Into the Maelstrom story:\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the .txt file in your Downloads folder\n",
    "    file_path = '/Users/hannahmarr/Downloads/A_DESCENT_INTO_THE_MAELSTROM.txt'\n",
    "\n",
    "    # Read the story text into the 'paragraph' variable\n",
    "    paragraph = read_story_from_file(file_path)\n",
    "\n",
    "    # Decompose and tokenize the story text\n",
    "    all_tagged_sentences = decompose_story(paragraph)\n",
    "\n",
    "    # Output the first tagged sentence to verify\n",
    "    print(all_tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2a6b5-b3f5-4217-8ba1-4e212b8cb9fb",
   "metadata": {},
   "source": [
    "3. [11 points] Tag all remaining words in the story as parts of speech using the Penn POS Tags. This SO answer shows how to obtain the POS tag values. Create and print a dictionary with the Penn POS Tags as keys and a list of words as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1414a27d-d22b-4ad9-bf3b-c967cfe2331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hannahmarr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hannahmarr/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK data (punkt for tokenization and averaged_perceptron_tagger for POS tagging)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to read the story text from a local file\n",
    "def read_story_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Define the list of allowed Penn POS tags\n",
    "allowed_pos_tags = {\n",
    "    'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD',\n",
    "    'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR',\n",
    "    'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ',\n",
    "    'WDT', 'WP', 'WP$', 'WRB'\n",
    "}\n",
    "\n",
    "# Function to tag words and create a dictionary of POS tags with corresponding words\n",
    "def tag_words_by_pos(paragraph):\n",
    "    # Tokenize the paragraph into sentences\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    \n",
    "    # Initialize a dictionary with POS tags as keys and list of words as values\n",
    "    pos_dict = defaultdict(list)\n",
    "    \n",
    "    # Loop through each sentence, tokenize and tag it\n",
    "    for sentence in sentences:\n",
    "        words_with_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        for word, tag in words_with_tags:\n",
    "            # Only include words with POS tags that are in the allowed_pos_tags list\n",
    "            if tag in allowed_pos_tags:\n",
    "                pos_dict[tag].append(word)\n",
    "    \n",
    "    return pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "89811eca-abaf-44ca-bbc0-0249c49f7b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: ['nature', 'commensurate', 'unsearchableness', 'speak', 'mortal', 'suppose', 'single', 'jetty', 'black', 'white']\n",
      "NN: ['providence', 'vastness', 'profundity', 'democritus', 'joseph', 'glanville', 'summit', 'crag', 'man', 'length']\n",
      "RB: ['depth', 'long', 'ago', 'deadly', 'scarcely', 'carelessly', 'beneath', 'deeply', 'length', 'upward']\n",
      "VB: ['raise', 'timid', 'morrow', 'watch', 'deck', 'elder', 'shake', 'slack', 'keel', 'hold']\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the .txt file in your Downloads folder\n",
    "    file_path = '/Users/hannahmarr/Downloads/A_DESCENT_INTO_THE_MAELSTROM_CLEANED.txt'\n",
    "    \n",
    "    # Step 5: Read the story text into the 'paragraph' variable\n",
    "    paragraph = read_story_from_file(file_path)\n",
    "    \n",
    "    # Step 6: Tag words and build the POS dictionary\n",
    "    pos_tagged_dict = tag_words_by_pos(paragraph)\n",
    "    \n",
    "    # Step 7: Print the dictionary to verify the POS tags and associated words\n",
    "    for pos_tag, words in pos_tagged_dict.items():\n",
    "        print(f\"{pos_tag}: {words[:10]}\")  # Print first 10 words for each POS tag to keep the output concise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce63749-df8d-461b-a8cb-2a724d55dff6",
   "metadata": {},
   "source": [
    "4. [11 points] In this framework, each row will represent a story. The columns will be as follows:\n",
    "\n",
    "The text of the story,\n",
    "\n",
    "Two-letter prefixes of each tag, for example NN, VB, RB, JJ etc.and the words belonging to that tag in the story. \n",
    "\n",
    "Show your code and the tag columns, at least for the one story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "85afab99-af2c-4ecb-bd72-947b1ac5f685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hannahmarr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hannahmarr/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define the list of allowed Penn POS tags\n",
    "allowed_pos_tags = {\n",
    "    'NN', 'VB', 'JJ', 'RB'\n",
    "}\n",
    "\n",
    "# Function to read the story text from a local file\n",
    "def read_story_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Step 3: Function to tag words and create a dictionary of POS tags with corresponding words\n",
    "def tag_words_by_pos(paragraph):\n",
    "    # Tokenize the paragraph into sentences\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    \n",
    "    # Initialize a dictionary with POS tags as keys and list of words as values\n",
    "    pos_dict = defaultdict(list)\n",
    "    \n",
    "    # Loop through each sentence, tokenize and tag it\n",
    "    for sentence in sentences:\n",
    "        words_with_tags = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        for word, tag in words_with_tags:\n",
    "            # Only include words with POS tags that are in the allowed_pos_tags list\n",
    "            if tag in allowed_pos_tags:\n",
    "                pos_dict[tag].append(word)\n",
    "    \n",
    "    return pos_dict\n",
    "\n",
    "# Step 4: Create a DataFrame for the POS tag results\n",
    "def create_pos_dataframe(title, text, pos_dict):\n",
    "    # Prepare the base data with 'Title' and 'Text' columns\n",
    "    data = {\n",
    "        'Title': [title],\n",
    "        'Text': [text]\n",
    "    }\n",
    "    \n",
    "    # Loop through each allowed POS tag and add the corresponding words to the data\n",
    "    for tag in allowed_pos_tags:\n",
    "        data[tag] = [', '.join(pos_dict.get(tag, []))]  # Join the words with commas\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "19e8c73c-06db-468a-9552-84f692fceb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with A Descent Into the Maelstrom\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/Users/hannahmarr/Downloads/A_DESCENT_INTO_THE_MAELSTROM_CLEANED.txt'\n",
    "\n",
    "    # Read the story text into the 'paragraph' variable\n",
    "    paragraph = read_story_from_file(file_path)\n",
    "    \n",
    "    # Tag words and build the POS dictionary\n",
    "    pos_tagged_dict = tag_words_by_pos(paragraph)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = create_pos_dataframe(\"Maelstrom\", paragraph[0:100], pos_tagged_dict) # paragraph[0:100] to not dominate the dataframe with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "d4f49c99-7f15-4eb5-90c2-13c5057aa938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>NN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>VB</th>\n",
       "      <th>RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maelstrom</td>\n",
       "      <td>ways god nature providence ways models frame commensurate vastness profundity unsearchableness works</td>\n",
       "      <td>providence, vastness, profundity, democritus, joseph, glanville, summit, crag, man, length, route, event, man, man, body, soul, man, day, change, weaken, exertion, giddy, cliff, edge, rest, portion, body, tenure, slippery, edge, cliff, sheer, precipice, rock, dozen, truth, position, companion, ground, clung, shrubs, glance, sky, idea, fury, reason, courage, sit, distance, view, scene, event, story, spot, eye, manner, coast, latitude, province, nordland, district, lofoden, mountain, sit, helseggen, cloudy, hold, grass, feel, belt, vapor, beneath, sea, expanse, hue, bring, mind, geographer, account, mare, tenebrarum, panorama, imagination, conceive, eye, reach, character, gloom, surf, promontory, apex, distance, bleak, island, position, wilderness, surge, size, cluster, dark, appearance, space, distant, island, time, gale, brig, lay, reefed, trysail, sight, regular, quick, cross, water, direction, wind, foam, vicinity, distance, man, mile, islesen, hotholm, buckholm, moskoe, vurrgh, change, water, caught, glimpse, sea, summit, man, sound, herd, moment, term, character, beneath, velocity, moment, speed, impetuosity, fury, moskoe, coast, bed, conflicting, convulsion, rapidity, water, alteration, surface, distance, combination, gyratory, motion, germ, vast, definite, existence, circle, mile, diameter, edge, whirl, belt, spray, particle, mouth, funnel, interior, eye, fathom, jet, wall, water, horizon, angle, round, motion, voice, half, half, roar, mighty, cataract, mountain, base, rock, face, clung, herbage, agitation, length, man, maelstr, island, moskoe, midway, impart, conception, magnificence, horror, scene, sense, point, view, writer, question, time, summit, helseggen, storm, description, impression, spectacle, moskoe, depth, water, thirty, vurrgh, depth, passage, vessel, risk, stream, country, rapidity, ebb, sea, scarce, ship, attraction, beat, water, relaxes, tranquility, flood, calm, quarter, hour, violence, stream, fury, reach, stream, violence, moskoe, borne, pine, rise, broken, degree, craggy, stream, reflux, sea, water, year, morning, sexagesima, sunday, impetuosity, water, vicinity, vortex, reference, moskoe, depth, centre, moskoe, str, proof, fact, glance, abyss, whirl, crag, helseggen, pinnacle, phlegethon, simplicity, jonas, ramus, belief, fact, thing, ship, existence, influence, resist, hurricane, remember, perusal, aspect, idea, collision, reflux, ridge, water, flood, fall, result, whirlpool, vortex, suction, lesser, dia, britannica, imagine, centre, channel, maelstr, globe, gulf, bothnia, instance, opinion, imagination, guide, view, notion, inability, paper, absurd, thunder, abyss, whirl, man, round, crag, lee, roar, water, story, convince, moskoe, str, smack, seventy, habit, fishing, fishing, business, southward, fish, risk, choice, variety, abundance, day, craft, scrape, week, fact, matter, speculation, risk, life, labor, courage, capital, smack, cove, coast, practice, fine, weather, advantage, push, channel, moskoe, pool, drop, anchorage, sandflesen, time, water, expedition, wind, return, seldom, calculation, point, night, anchor, account, calm, thing, remain, week, death, gale, occasion, sea, spite, round, anchor, cross, day, luck, spot, weather, shift, gauntlet, moskoe, accident, heart, mouth, slack, wind, smack, brother, son, assistance, risk, heart, danger, danger, truth, day, day, hurricane, morning, afternoon, breeze, south, sun, shone, seaman, foreseen, clock, fish, day, str, slack, water, wind, quarter, time, rate, danger, reason, aback, breeze, helseggen, boat, wind, headway, return, anchorage, horizon, copper, cloud, velocity, breeze, direction, state, time, minute, storm, sky, spray, hurricane, seaman, thing, board, brother, safety, boat, thing, water, flush, hatch, bow, hatch, custom, str, precaution, circumstance, lay, brother, destruction, opportunity, threw, deck, bow, foot, fore, mast, instinct, thing, time, breath, clung, bolt, stand, dog, water, measure, arm, elder, brother, heart, joy, moment, joy, ear, word, moskoe, moment, shook, head, foot, ague, word, understand, wind, crossing, channel, calmest, weather, wait, watch, pool, hurricane, moment, dream, hope, time, fury, spent, feel, change, direction, pitch, overhead, burst, rift, sky, bright, moon, lustre, thing, distinctness, scene, brother, manner, din, word, voice, head, death, thought, watch, fob, face, moonlight, burst, clock, time, slack, whirl, fury, boat, laden, slip, beneath, landsman, sea, phrase, ridden, sea, bore, rise, sweep, slide, plunge, feel, dizzy, mountain, dream, glance, glance, position, instant, moskoe, quarter, mile, day, whirl, race, place, horror, spasm, boat, half, direction, thunderbolt, moment, water, kind, shrill, waste, steam, steam, belt, surf, moment, plunge, abyss, velocity, boat, sink, water, air, surface, surge, whirl, larboard, wall, horizon, mind, hope, deal, terror, despair, truth, thing, die, manner, paltry, consideration, life, view, manifestation, god, power, idea, mind, curiosity, whirl, sacrifice, grief, man, mind, extremity, boat, pool, light, circumstance, possession, cessation, reach, situation, surf, bed, ocean, ridge, sea, form, idea, confusion, mind, spray, blind, deafen, strangle, power, action, reflection, measure, death, prison, doom, circuit, belt, round, hour, surge, nearer, nearer, edge, time, bolt, brother, water, cask, thing, deck, brink, pit, agony, terror, force, secure, grasp, grief, attempt, sheer, care, contest, point, difference, cask, difficulty, smack, sweeps, position, starboard, prayer, god, descent, hold, barrel, destruction, death, water, moment, moment, sense, motion, vessel, exception, courage, scene, forget, horror, admiration, boat, surface, funnel, circumference, depth, bewildering, rapidity, spun, radiance, shot, rift, glory, observe, burst, terrific, grandeur, beheld, gaze, direction, view, manner, smack, hung, pool, keel, deck, plane, water, angle, beam, difficulty, situation, level, speed, search, profound, gulf, mist, magnificent, rainbow, bridge, time, eternity, mist, spray, doubt, yell, attempt, slide, abyss, distance, slope, proportionate, round, round, movement, circuit, whirl, progress, revolution, waste, ebony, borne, boat, object, embrace, whirl, house, furniture, curiosity, place, drew, nearer, nearer, doom, company, amusement, time, thing, plunge, wreck, dutch, merchant, ship, overtook, length, making, fact, fact, miscalculation, train, reflection, heart, dawn, hope, memory, observation, variety, matter, moskoe, str, number, chafed, account, difference, supposing, whirl, period, reason, reach, turn, flood, ebb, case, instance, fate, rule, shape, superiority, speed, descent, size, shape, cylinder, school, master, district, explanation, fact, consequence, vortex, resistance, suction, drawn, difficulty, body, form, circumstance, turn, account, revolution, barrel, yard, mast, level, station, lash, water, cask, counter, throw, water, attention, power, length, design, case, shook, head, station, reach, emergency, delay, bitter, struggle, fate, sea, moment, hesitation, result, escape, possession, mode, escape, anticipate, story, conclusion, hour, smack, distance, beneath, succession, chaos, barrel, sunk, half, distance, gulf, spot, change, place, character, whirlpool, slope, rainbow, sky, view, spot, pool, moskoe, hour, slack, sea, str, fatigue, danger, memory, horror, board, traveller, spirit, land, hair, day, expression, countenance, story, faith</td>\n",
       "      <td>nature, commensurate, unsearchableness, speak, mortal, suppose, single, jetty, black, white, unstring, tremble, cliff, thrown, extreme, unobstructed, black, sixteen, half, perilous, vain, danger, long, sufficient, brought, close, norwegian, eighth, degree, great, dreary, giddy, wide, ocean, inky, nubian, desolate, human, outstretched, black, cliff, high, white, opposite, visible, small, discernible, arose, craggy, ocean, unusual, strong, landward, double, hull, short, angry, moskoe, ambaaren, suarven, stockholm, true, understand, hear, interior, lofoden, burst, aware, loud, vast, american, prairie, seamen, ocean, current, gazed, current, monstrous, headlong, ungovernable, main, uproar, vast, scarred, gigantic, innumerable, eastward, precipitous, radical, general, smooth, prodigious, apparent, great, form, distinct, broad, terrific, smooth, black, shriek, niagara, agony, threw, scant, nervous, great, whirlpool, moskoe, str, ordinary, prepared, circumstantial, wild, feeble, lofoden, afford, convenient, flood, lofoden, boisterous, impetuous, dreadful, depth, carried, thrown, ebb, weather, boisterous, dangerous, mile, impossible, fruitless, bear, swim, lofoden, stream, large, current, torn, consist, fro, flux, high, low, noise, depth, shore, lofoden, sidelong, difficult, evident, attraction, phenomenon, plausible, unsatisfactory, flux, natural, prodigious, remote, idle, hear, subject, conclusive, unintelligible, good, deaden, proceeded, vurrgh, violent, good, proper, attempt, lofoden, regular, usual, great, preferred, single, desperate, main, str, otterholm, remain, slack, steady, fail, mis, stay, dead, rare, arrival, boisterous, round, fouled, innumerable, lee, good, twentieth, bad, good, str, minute, strong, current, unmanageable, eighteen, great, young, horrible, tenth, blew, terrible, late, steady, follow, smack, fine, plenty, fresh, starboard, great, unusual, feel, uneasy, astern, singular, amazing, dead, long, smack, attempt, experienced, feather, complete, small, cross, seas, foresail, flat, narrow, gunwale, bolt, mere, undoubtedly, hold, clear, rid, collect, grasp, mouth, close, str, violent, fit, meant, wished, whirl, perceive, str, whirl, hope, great, fool, gun, ship, seas, lay, flat, absolute, singular, black, circular, clear, clear, deep, blue, wear, lit, god, light, understand, hear, single, shook, pale, hideous, dragged, ocean, str, deep, strong, gale, large, strange, gigantic, sky, high, sick, lofty, quick, sufficient, exact, str, whirlpool, dead, str, foam, sharp, larboard, shot, noise, sound, imagine, whirl, thought, amazing, borne, bubble, starboard, ocean, stood, huge, strange, rid, great, suppose, strung, reflect, magnificent, foolish, individual, wonderful, shame, depths, principal, singular, general, high, black, mountainous, heavy, gale, great, rid, petty, forbidden, uncertain, impossible, middle, horrible, stern, small, coop, counter, gale, large, afford, madman, maniac, bolt, astern, great, fro, immense, lurch, headlong, hurried, sweep, open, instant, lived, foam, magic, interior, vast, prodigious, smooth, ebony, circular, flood, golden, black, inmost, general, downward, unobstructed, surface, parallel, footing, dead, thick, hung, narrow, great, dare, foam, great, descent, uniform, complete, downward, perceptible, wide, liquid, visible, large, timber, unnatural, original, grow, dreadful, watch, strange, numerous, delirious, relative, fir, tree, awful, invariable, tremble, terror, great, buoyant, coast, lofoden, thrown, extraordinary, stuck, disfigured, entered, late, level, ocean, general, descent, equal, extent, spherical, sphere, equal, cylindrical, escape, subject, forgotten, natural, bulky, great, anxious, vessel, high, original, resolved, loose, impossible, cask, tale, bring, vast, wild, rapid, foam, great, vast, steep, violent, gulf, uprise, clear, surface, ocean, mountainous, coast, exhausted, speechless, daily, raven, black, white, told, merry</td>\n",
       "      <td>raise, timid, morrow, watch, deck, elder, shake, slack, keel, hold, slow, channel</td>\n",
       "      <td>depth, long, ago, deadly, scarcely, carelessly, beneath, deeply, length, upward, dizzily, deplorably, horridly, forcibly, ghastly, forever, properly, nearer, hideously, constantly, midway, northward, gradually, rapidly, eastward, vurrgh, sway, suddenly, suddenly, suddenly, dizzily, heaven, excess, exceedingly, weather, moskoe, noise, heard, inevitably, gradually, storm, norway, likewise, frequently, terribly, shore, plainly, constantly, early, ground, regard, close, immeasurably, deadly, bodily, generally, decidedly, universally, altogether, schooner, shortly, violently, stout, afterward, brightly, suddenly, folly, norway, cleverly, mainmast, completely, presently, overboard, horror, bound, long, carefully, slack, ear, presently, properly, cleverly, presently, ahead, involuntarily, afterward, suddenly, subside, completely, indistinctly, positively, considerably, gradually, securely, overboard, steadily, scarcely, abyss, felt, instinctively, midway, perfectly, ghastly, accurately, instinctively, scarcely, distinctly, nature, heavily, arose, partly, partly, mind, appearance, distinctly, completely, slowly, early, rapidly, slowly, sphere, equally, longer, securely, despairingly, counter, precisely, brother, headlong, forever, farther, overboard, momently, gradually, slowly, moon, radiantly, borne, violently, scarcely</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Title                                                                                                  Text  \\\n",
       "0  Maelstrom  ways god nature providence ways models frame commensurate vastness profundity unsearchableness works   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NN  \\\n",
       "0  providence, vastness, profundity, democritus, joseph, glanville, summit, crag, man, length, route, event, man, man, body, soul, man, day, change, weaken, exertion, giddy, cliff, edge, rest, portion, body, tenure, slippery, edge, cliff, sheer, precipice, rock, dozen, truth, position, companion, ground, clung, shrubs, glance, sky, idea, fury, reason, courage, sit, distance, view, scene, event, story, spot, eye, manner, coast, latitude, province, nordland, district, lofoden, mountain, sit, helseggen, cloudy, hold, grass, feel, belt, vapor, beneath, sea, expanse, hue, bring, mind, geographer, account, mare, tenebrarum, panorama, imagination, conceive, eye, reach, character, gloom, surf, promontory, apex, distance, bleak, island, position, wilderness, surge, size, cluster, dark, appearance, space, distant, island, time, gale, brig, lay, reefed, trysail, sight, regular, quick, cross, water, direction, wind, foam, vicinity, distance, man, mile, islesen, hotholm, buckholm, moskoe, vurrgh, change, water, caught, glimpse, sea, summit, man, sound, herd, moment, term, character, beneath, velocity, moment, speed, impetuosity, fury, moskoe, coast, bed, conflicting, convulsion, rapidity, water, alteration, surface, distance, combination, gyratory, motion, germ, vast, definite, existence, circle, mile, diameter, edge, whirl, belt, spray, particle, mouth, funnel, interior, eye, fathom, jet, wall, water, horizon, angle, round, motion, voice, half, half, roar, mighty, cataract, mountain, base, rock, face, clung, herbage, agitation, length, man, maelstr, island, moskoe, midway, impart, conception, magnificence, horror, scene, sense, point, view, writer, question, time, summit, helseggen, storm, description, impression, spectacle, moskoe, depth, water, thirty, vurrgh, depth, passage, vessel, risk, stream, country, rapidity, ebb, sea, scarce, ship, attraction, beat, water, relaxes, tranquility, flood, calm, quarter, hour, violence, stream, fury, reach, stream, violence, moskoe, borne, pine, rise, broken, degree, craggy, stream, reflux, sea, water, year, morning, sexagesima, sunday, impetuosity, water, vicinity, vortex, reference, moskoe, depth, centre, moskoe, str, proof, fact, glance, abyss, whirl, crag, helseggen, pinnacle, phlegethon, simplicity, jonas, ramus, belief, fact, thing, ship, existence, influence, resist, hurricane, remember, perusal, aspect, idea, collision, reflux, ridge, water, flood, fall, result, whirlpool, vortex, suction, lesser, dia, britannica, imagine, centre, channel, maelstr, globe, gulf, bothnia, instance, opinion, imagination, guide, view, notion, inability, paper, absurd, thunder, abyss, whirl, man, round, crag, lee, roar, water, story, convince, moskoe, str, smack, seventy, habit, fishing, fishing, business, southward, fish, risk, choice, variety, abundance, day, craft, scrape, week, fact, matter, speculation, risk, life, labor, courage, capital, smack, cove, coast, practice, fine, weather, advantage, push, channel, moskoe, pool, drop, anchorage, sandflesen, time, water, expedition, wind, return, seldom, calculation, point, night, anchor, account, calm, thing, remain, week, death, gale, occasion, sea, spite, round, anchor, cross, day, luck, spot, weather, shift, gauntlet, moskoe, accident, heart, mouth, slack, wind, smack, brother, son, assistance, risk, heart, danger, danger, truth, day, day, hurricane, morning, afternoon, breeze, south, sun, shone, seaman, foreseen, clock, fish, day, str, slack, water, wind, quarter, time, rate, danger, reason, aback, breeze, helseggen, boat, wind, headway, return, anchorage, horizon, copper, cloud, velocity, breeze, direction, state, time, minute, storm, sky, spray, hurricane, seaman, thing, board, brother, safety, boat, thing, water, flush, hatch, bow, hatch, custom, str, precaution, circumstance, lay, brother, destruction, opportunity, threw, deck, bow, foot, fore, mast, instinct, thing, time, breath, clung, bolt, stand, dog, water, measure, arm, elder, brother, heart, joy, moment, joy, ear, word, moskoe, moment, shook, head, foot, ague, word, understand, wind, crossing, channel, calmest, weather, wait, watch, pool, hurricane, moment, dream, hope, time, fury, spent, feel, change, direction, pitch, overhead, burst, rift, sky, bright, moon, lustre, thing, distinctness, scene, brother, manner, din, word, voice, head, death, thought, watch, fob, face, moonlight, burst, clock, time, slack, whirl, fury, boat, laden, slip, beneath, landsman, sea, phrase, ridden, sea, bore, rise, sweep, slide, plunge, feel, dizzy, mountain, dream, glance, glance, position, instant, moskoe, quarter, mile, day, whirl, race, place, horror, spasm, boat, half, direction, thunderbolt, moment, water, kind, shrill, waste, steam, steam, belt, surf, moment, plunge, abyss, velocity, boat, sink, water, air, surface, surge, whirl, larboard, wall, horizon, mind, hope, deal, terror, despair, truth, thing, die, manner, paltry, consideration, life, view, manifestation, god, power, idea, mind, curiosity, whirl, sacrifice, grief, man, mind, extremity, boat, pool, light, circumstance, possession, cessation, reach, situation, surf, bed, ocean, ridge, sea, form, idea, confusion, mind, spray, blind, deafen, strangle, power, action, reflection, measure, death, prison, doom, circuit, belt, round, hour, surge, nearer, nearer, edge, time, bolt, brother, water, cask, thing, deck, brink, pit, agony, terror, force, secure, grasp, grief, attempt, sheer, care, contest, point, difference, cask, difficulty, smack, sweeps, position, starboard, prayer, god, descent, hold, barrel, destruction, death, water, moment, moment, sense, motion, vessel, exception, courage, scene, forget, horror, admiration, boat, surface, funnel, circumference, depth, bewildering, rapidity, spun, radiance, shot, rift, glory, observe, burst, terrific, grandeur, beheld, gaze, direction, view, manner, smack, hung, pool, keel, deck, plane, water, angle, beam, difficulty, situation, level, speed, search, profound, gulf, mist, magnificent, rainbow, bridge, time, eternity, mist, spray, doubt, yell, attempt, slide, abyss, distance, slope, proportionate, round, round, movement, circuit, whirl, progress, revolution, waste, ebony, borne, boat, object, embrace, whirl, house, furniture, curiosity, place, drew, nearer, nearer, doom, company, amusement, time, thing, plunge, wreck, dutch, merchant, ship, overtook, length, making, fact, fact, miscalculation, train, reflection, heart, dawn, hope, memory, observation, variety, matter, moskoe, str, number, chafed, account, difference, supposing, whirl, period, reason, reach, turn, flood, ebb, case, instance, fate, rule, shape, superiority, speed, descent, size, shape, cylinder, school, master, district, explanation, fact, consequence, vortex, resistance, suction, drawn, difficulty, body, form, circumstance, turn, account, revolution, barrel, yard, mast, level, station, lash, water, cask, counter, throw, water, attention, power, length, design, case, shook, head, station, reach, emergency, delay, bitter, struggle, fate, sea, moment, hesitation, result, escape, possession, mode, escape, anticipate, story, conclusion, hour, smack, distance, beneath, succession, chaos, barrel, sunk, half, distance, gulf, spot, change, place, character, whirlpool, slope, rainbow, sky, view, spot, pool, moskoe, hour, slack, sea, str, fatigue, danger, memory, horror, board, traveller, spirit, land, hair, day, expression, countenance, story, faith   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         JJ  \\\n",
       "0  nature, commensurate, unsearchableness, speak, mortal, suppose, single, jetty, black, white, unstring, tremble, cliff, thrown, extreme, unobstructed, black, sixteen, half, perilous, vain, danger, long, sufficient, brought, close, norwegian, eighth, degree, great, dreary, giddy, wide, ocean, inky, nubian, desolate, human, outstretched, black, cliff, high, white, opposite, visible, small, discernible, arose, craggy, ocean, unusual, strong, landward, double, hull, short, angry, moskoe, ambaaren, suarven, stockholm, true, understand, hear, interior, lofoden, burst, aware, loud, vast, american, prairie, seamen, ocean, current, gazed, current, monstrous, headlong, ungovernable, main, uproar, vast, scarred, gigantic, innumerable, eastward, precipitous, radical, general, smooth, prodigious, apparent, great, form, distinct, broad, terrific, smooth, black, shriek, niagara, agony, threw, scant, nervous, great, whirlpool, moskoe, str, ordinary, prepared, circumstantial, wild, feeble, lofoden, afford, convenient, flood, lofoden, boisterous, impetuous, dreadful, depth, carried, thrown, ebb, weather, boisterous, dangerous, mile, impossible, fruitless, bear, swim, lofoden, stream, large, current, torn, consist, fro, flux, high, low, noise, depth, shore, lofoden, sidelong, difficult, evident, attraction, phenomenon, plausible, unsatisfactory, flux, natural, prodigious, remote, idle, hear, subject, conclusive, unintelligible, good, deaden, proceeded, vurrgh, violent, good, proper, attempt, lofoden, regular, usual, great, preferred, single, desperate, main, str, otterholm, remain, slack, steady, fail, mis, stay, dead, rare, arrival, boisterous, round, fouled, innumerable, lee, good, twentieth, bad, good, str, minute, strong, current, unmanageable, eighteen, great, young, horrible, tenth, blew, terrible, late, steady, follow, smack, fine, plenty, fresh, starboard, great, unusual, feel, uneasy, astern, singular, amazing, dead, long, smack, attempt, experienced, feather, complete, small, cross, seas, foresail, flat, narrow, gunwale, bolt, mere, undoubtedly, hold, clear, rid, collect, grasp, mouth, close, str, violent, fit, meant, wished, whirl, perceive, str, whirl, hope, great, fool, gun, ship, seas, lay, flat, absolute, singular, black, circular, clear, clear, deep, blue, wear, lit, god, light, understand, hear, single, shook, pale, hideous, dragged, ocean, str, deep, strong, gale, large, strange, gigantic, sky, high, sick, lofty, quick, sufficient, exact, str, whirlpool, dead, str, foam, sharp, larboard, shot, noise, sound, imagine, whirl, thought, amazing, borne, bubble, starboard, ocean, stood, huge, strange, rid, great, suppose, strung, reflect, magnificent, foolish, individual, wonderful, shame, depths, principal, singular, general, high, black, mountainous, heavy, gale, great, rid, petty, forbidden, uncertain, impossible, middle, horrible, stern, small, coop, counter, gale, large, afford, madman, maniac, bolt, astern, great, fro, immense, lurch, headlong, hurried, sweep, open, instant, lived, foam, magic, interior, vast, prodigious, smooth, ebony, circular, flood, golden, black, inmost, general, downward, unobstructed, surface, parallel, footing, dead, thick, hung, narrow, great, dare, foam, great, descent, uniform, complete, downward, perceptible, wide, liquid, visible, large, timber, unnatural, original, grow, dreadful, watch, strange, numerous, delirious, relative, fir, tree, awful, invariable, tremble, terror, great, buoyant, coast, lofoden, thrown, extraordinary, stuck, disfigured, entered, late, level, ocean, general, descent, equal, extent, spherical, sphere, equal, cylindrical, escape, subject, forgotten, natural, bulky, great, anxious, vessel, high, original, resolved, loose, impossible, cask, tale, bring, vast, wild, rapid, foam, great, vast, steep, violent, gulf, uprise, clear, surface, ocean, mountainous, coast, exhausted, speechless, daily, raven, black, white, told, merry   \n",
       "\n",
       "                                                                                  VB  \\\n",
       "0  raise, timid, morrow, watch, deck, elder, shake, slack, keel, hold, slow, channel   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        RB  \n",
       "0  depth, long, ago, deadly, scarcely, carelessly, beneath, deeply, length, upward, dizzily, deplorably, horridly, forcibly, ghastly, forever, properly, nearer, hideously, constantly, midway, northward, gradually, rapidly, eastward, vurrgh, sway, suddenly, suddenly, suddenly, dizzily, heaven, excess, exceedingly, weather, moskoe, noise, heard, inevitably, gradually, storm, norway, likewise, frequently, terribly, shore, plainly, constantly, early, ground, regard, close, immeasurably, deadly, bodily, generally, decidedly, universally, altogether, schooner, shortly, violently, stout, afterward, brightly, suddenly, folly, norway, cleverly, mainmast, completely, presently, overboard, horror, bound, long, carefully, slack, ear, presently, properly, cleverly, presently, ahead, involuntarily, afterward, suddenly, subside, completely, indistinctly, positively, considerably, gradually, securely, overboard, steadily, scarcely, abyss, felt, instinctively, midway, perfectly, ghastly, accurately, instinctively, scarcely, distinctly, nature, heavily, arose, partly, partly, mind, appearance, distinctly, completely, slowly, early, rapidly, slowly, sphere, equally, longer, securely, despairingly, counter, precisely, brother, headlong, forever, farther, overboard, momently, gradually, slowly, moon, radiantly, borne, violently, scarcely  "
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the dataframe using Jupyter's built-in format for better readability\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6baf7-6b3a-47fb-9705-bd90f63734e9",
   "metadata": {},
   "source": [
    "[12 points] The conjecture of many linguists is that the number of different parts of speech per thousand words, (nouns, verbs, adjectives, adverbs, …). is pretty much the same for all stories in a given language. In this case, with all stories in English, and all from the same author, we expect it to be true. Is the conjecture consistent with your findings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862624f-ec55-4501-920c-384421bc362b",
   "metadata": {},
   "source": [
    "The conjecture suggests that the distribution of parts of speech (POS) is roughly uniform across stories in a given language. We can mathematically express this as:\n",
    "\n",
    "POS Frequency Ratio = (Number of words for a specific POS / Total words in the story) × 1000\n",
    "\n",
    "For each story, POS Frequency Ratio (for each POS tag like NN, VB, JJ, etc.) should be relatively constant. We can compute the ratio for each part of speech per thousand words.\n",
    "\n",
    "My plan for mathematically evaluating this is as follows:\n",
    "\n",
    "POS Tag Frequency: For each story, we calculate the number of occurrences of each POS tag (e.g., NN, VB, JJ).\n",
    "\n",
    "Total Word Count: Calculate the total number of words in each story.\n",
    "\n",
    "POS Frequency Ratio: Compute the POS frequency ratio (per thousand words) for each POS tag.\n",
    "\n",
    "Consistency Across Stories: Analyze how the ratios vary between stories. A consistent pattern would support the conjecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "51995541-660a-46ae-90c5-3eecafbab4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have the POS-tagged words for each story in a DataFrame\n",
    "# We can compute the frequency of each POS tag and the total word count for each story\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_pos_frequencies(pos_dict, total_words):\n",
    "    # Initialize a dictionary to store frequencies per 1000 words for each POS tag\n",
    "    pos_frequencies = {}\n",
    "    \n",
    "    for pos, words in pos_dict.items():\n",
    "        pos_frequencies[pos] = (len(words) / total_words) * 1000  # Frequency per 1000 words\n",
    "    \n",
    "    return pos_frequencies\n",
    "\n",
    "# Example usage (with existing DataFrame 'df' from previous steps)\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: for one story (expand this for multiple stories)\n",
    "    total_words = sum(len(pos_tagged_dict.get(tag, [])) for tag in allowed_pos_tags)  # Total words in the story\n",
    "    \n",
    "    # Calculate POS frequency ratios for the story\n",
    "    pos_frequencies = calculate_pos_frequencies(pos_tagged_dict, total_words)\n",
    "    \n",
    "    # Convert the POS frequencies to a DataFrame for easier comparison across stories\n",
    "    pos_freq_df = pd.DataFrame([pos_frequencies], index=['Maelstrom'])  # 'Maelstrom' is the example story title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "59d38fdf-fd45-4d21-9de9-58794aefaf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JJ</th>\n",
       "      <th>NN</th>\n",
       "      <th>RB</th>\n",
       "      <th>VB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Maelstrom</th>\n",
       "      <td>304.485155</td>\n",
       "      <td>599.49463</td>\n",
       "      <td>88.439672</td>\n",
       "      <td>7.580543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   JJ         NN         RB        VB\n",
       "Maelstrom  304.485155  599.49463  88.439672  7.580543"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the POS frequency ratios (per 1000 words) for the story using Jupyter's built-in formatting for better viewability\n",
    "pos_freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033d68c-1322-4f17-a220-aaf7c35b8887",
   "metadata": {},
   "source": [
    "If we wanted to compare across multiple stories, we could repeat the process for each story and compile the results into a single DataFrame where each row represents a story and each column represents the POS frequency ratio for a specific part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "e1639546-b3ac-40bf-a8e2-d7e949645134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dicebant mihi sodales sepulchrum amicae visitarem curas meas aliquar tulum fore levatas ebn zaiat misery manifold wretchedness earth multiform overreaching wide horizon rainbow hues hues arch distinct intimately blended overreaching wide horizon rainbow beauty derived type unloveliness covenant peace simile sorrow ethics evil consequence good fact joy sorrow born memory bliss anguish day agonies origin ecstasies baptismal egaeus family mention towers land time honored gloomy gray hereditary hall\n"
     ]
    }
   ],
   "source": [
    "# Here I will compare across five different stories to get a better sense of parts per speech distribution\n",
    "# Berenice\n",
    "# Read and clean the file text\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path of the downloaded text file\n",
    "    file_path = '/Users/hannahmarr/Downloads/BERENICE.txt'\n",
    "    cleaned_file_path = '/Users/hannahmarr/Downloads/BERENICE_CLEANED.txt'  # Path where cleaned file will be saved\n",
    "\n",
    "    # Read the original text\n",
    "    original_text = read_file(file_path)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(original_text)\n",
    "\n",
    "     # Step 5: Save the cleaned text (optional)\n",
    "    save_cleaned_text(cleaned_file_path, cleaned_text)\n",
    "\n",
    "    # Output the first 500 characters of cleaned text to verify\n",
    "    print(cleaned_text[:500])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/Users/hannahmarr/Downloads/BERENICE_CLEANED.txt'\n",
    "\n",
    "    # Read the story text into the 'paragraph' variable\n",
    "    paragraph = read_story_from_file(file_path)\n",
    "    \n",
    "    # Tag words and build the POS dictionary\n",
    "    pos_tagged_dict = tag_words_by_pos(paragraph)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df2 = create_pos_dataframe(\"Berenice\", paragraph[0:100], pos_tagged_dict) # paragraph[0:100] to not dominate the dataframe with text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: for one story (expand this for multiple stories)\n",
    "    total_words = sum(len(pos_tagged_dict.get(tag, [])) for tag in allowed_pos_tags)  # Total words in the story\n",
    "    \n",
    "    # Calculate POS frequency ratios for the story\n",
    "    pos_frequencies = calculate_pos_frequencies(pos_tagged_dict, total_words)\n",
    "    \n",
    "    # Convert the POS frequencies to a DataFrame for easier comparison across stories\n",
    "    pos_freq_df2 = pd.DataFrame([pos_frequencies], index=['Berenice'])  # 'Maelstrom' is the example story title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "651d47aa-2b38-4452-9493-ceeb4b450bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JJ</th>\n",
       "      <th>NN</th>\n",
       "      <th>VB</th>\n",
       "      <th>RB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Berenice</th>\n",
       "      <td>356.635071</td>\n",
       "      <td>541.469194</td>\n",
       "      <td>10.663507</td>\n",
       "      <td>91.232227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  JJ          NN         VB         RB\n",
       "Berenice  356.635071  541.469194  10.663507  91.232227"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the POS frequency ratios (per 1000 words) for the story using Jupyter's built-in formatting for better viewability\n",
    "pos_freq_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "0024cdbd-1a0a-4632-98ab-9e4a5acceb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "injuries fortunato borne ventured insult vowed revenge nature soul suppose utterance threat length avenged point definitively settled definitiveness resolved precluded idea risk punish punish impunity wrong unredressed retribution overtakes redresser equally unredressed avenger fails felt wrong understood word deed fortunato doubt good continued smile face perceive smile thought immolation weak point fortunato man respected feared prided connoisseurship wine italians true virtuoso spirit enthusi\n"
     ]
    }
   ],
   "source": [
    "# The Cask of Amontillado\n",
    "# Read and clean the file text\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path of the downloaded text file\n",
    "    file_path = '/Users/hannahmarr/Downloads/THE_CASK_OF_AMONTILLADO.txt'\n",
    "    cleaned_file_path = '/Users/hannahmarr/Downloads/THE_CASK_OF_AMONTILLADO_CLEANED.txt'  # Path where cleaned file will be saved\n",
    "\n",
    "    # Read the original text\n",
    "    original_text = read_file(file_path)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(original_text)\n",
    "\n",
    "     # Step 5: Save the cleaned text (optional)\n",
    "    save_cleaned_text(cleaned_file_path, cleaned_text)\n",
    "\n",
    "    # Output the first 500 characters of cleaned text to verify\n",
    "    print(cleaned_text[:500])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/Users/hannahmarr/Downloads/THE_CASK_OF_AMONTILLADO_CLEANED.txt'\n",
    "\n",
    "    # Read the story text into the 'paragraph' variable\n",
    "    paragraph = read_story_from_file(file_path)\n",
    "    \n",
    "    # Tag words and build the POS dictionary\n",
    "    pos_tagged_dict = tag_words_by_pos(paragraph)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df3 = create_pos_dataframe(\"Amontillado\", paragraph[0:100], pos_tagged_dict) # paragraph[0:100] to not dominate the dataframe with text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: for one story (expand this for multiple stories)\n",
    "    total_words = sum(len(pos_tagged_dict.get(tag, [])) for tag in allowed_pos_tags)  # Total words in the story\n",
    "    \n",
    "    # Calculate POS frequency ratios for the story\n",
    "    pos_frequencies = calculate_pos_frequencies(pos_tagged_dict, total_words)\n",
    "    \n",
    "    # Convert the POS frequencies to a DataFrame for easier comparison across stories\n",
    "    pos_freq_df3 = pd.DataFrame([pos_frequencies], index=['Amontillado'])  # 'Maelstrom' is the example story title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "145d8cf3-567b-45c1-9d11-0f3df199abea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JJ</th>\n",
       "      <th>NN</th>\n",
       "      <th>RB</th>\n",
       "      <th>VB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Amontillado</th>\n",
       "      <td>349.757224</td>\n",
       "      <td>558.544934</td>\n",
       "      <td>85.250338</td>\n",
       "      <td>6.447505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     JJ          NN         RB        VB\n",
       "Amontillado  349.757224  558.544934  85.250338  6.447505"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the POS frequency ratios (per 1000 words) for the story using Jupyter's built-in formatting for better viewability\n",
    "pos_freq_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "09e940d5-0ec4-4eca-b1bd-63f062d66df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wild homely narrative pen expect solicit belief mad expect case senses reject evidence mad surely dream morrow die day unburthen soul purpose place plainly succinctly comment series mere household events consequences events terrified tortured destroyed attempt expound presented horror terrible barroques intellect reduce phantasm common place intellect calm logical excitable perceive circumstances awe ordinary succession natural effects infancy docility humanity disposition tenderness heart consp\n"
     ]
    }
   ],
   "source": [
    "# The Black Cat\n",
    "# Read and clean the file text\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path of the downloaded text file\n",
    "    file_path = '/Users/hannahmarr/Downloads/THE_BLACK_CAT.txt'\n",
    "    cleaned_file_path = '/Users/hannahmarr/Downloads/THE_BLACK_CAT_CLEANED.txt'  # Path where cleaned file will be saved\n",
    "\n",
    "    # Read the original text\n",
    "    original_text = read_file(file_path)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(original_text)\n",
    "\n",
    "     # Step 5: Save the cleaned text (optional)\n",
    "    save_cleaned_text(cleaned_file_path, cleaned_text)\n",
    "\n",
    "    # Output the first 500 characters of cleaned text to verify\n",
    "    print(cleaned_text[:500])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/Users/hannahmarr/Downloads/THE_BLACK_CAT_CLEANED.txt'\n",
    "\n",
    "    # Read the story text into the 'paragraph' variable\n",
    "    paragraph = read_story_from_file(file_path)\n",
    "    \n",
    "    # Tag words and build the POS dictionary\n",
    "    pos_tagged_dict = tag_words_by_pos(paragraph)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df4 = create_pos_dataframe(\"Black_Cat\", paragraph[0:100], pos_tagged_dict) # paragraph[0:100] to not dominate the dataframe with text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: for one story (expand this for multiple stories)\n",
    "    total_words = sum(len(pos_tagged_dict.get(tag, [])) for tag in allowed_pos_tags)  # Total words in the story\n",
    "    \n",
    "    # Calculate POS frequency ratios for the story\n",
    "    pos_frequencies = calculate_pos_frequencies(pos_tagged_dict, total_words)\n",
    "    \n",
    "    # Convert the POS frequencies to a DataFrame for easier comparison across stories\n",
    "    pos_freq_df4 = pd.DataFrame([pos_frequencies], index=['Black_Cat'])  # 'Maelstrom' is the example story title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "05fb14e5-7df7-48f4-80d0-dd65a4228bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JJ</th>\n",
       "      <th>RB</th>\n",
       "      <th>NN</th>\n",
       "      <th>VB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Black_Cat</th>\n",
       "      <td>328.947368</td>\n",
       "      <td>77.935223</td>\n",
       "      <td>585.020243</td>\n",
       "      <td>8.097166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   JJ         RB          NN        VB\n",
       "Black_Cat  328.947368  77.935223  585.020243  8.097166"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the POS frequency ratios (per 1000 words) for the story using Jupyter's built-in formatting for better viewability\n",
    "pos_freq_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "82314e1d-963a-464d-aca5-819afa42112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "son luth suspendu sit touche sonne ranger dull dark soundless day autumn year clouds hung oppressively low heavens passing horseback singularly dreary tract country length shades evening drew view melancholy house usher glimpse building sense insufferable gloom pervaded spirit insufferable feeling unrelieved half pleasurable poetic sentiment mind receives sternest natural images desolate terrible looked scene mere house simple landscape features domain bleak walls vacant eye windows rank sedges \n"
     ]
    }
   ],
   "source": [
    "# The Fall of the House of Usher\n",
    "# Read and clean the file text\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path of the downloaded text file\n",
    "    file_path = '/Users/hannahmarr/Downloads/THE_FALL_OF_THE_HOUSE_OF_USHER.txt'\n",
    "    cleaned_file_path = '/Users/hannahmarr/Downloads/THE_FALL_OF_THE_HOUSE_OF_USHER_CLEANED.txt'  # Path where cleaned file will be saved\n",
    "\n",
    "    # Read the original text\n",
    "    original_text = read_file(file_path)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(original_text)\n",
    "\n",
    "     # Step 5: Save the cleaned text (optional)\n",
    "    save_cleaned_text(cleaned_file_path, cleaned_text)\n",
    "\n",
    "    # Output the first 500 characters of cleaned text to verify\n",
    "    print(cleaned_text[:500])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/Users/hannahmarr/Downloads/THE_FALL_OF_THE_HOUSE_OF_USHER.txt'\n",
    "\n",
    "    # Read the story text into the 'paragraph' variable\n",
    "    paragraph = read_story_from_file(file_path)\n",
    "    \n",
    "    # Tag words and build the POS dictionary\n",
    "    pos_tagged_dict = tag_words_by_pos(paragraph)\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df5 = create_pos_dataframe(\"House_of_Usher\", paragraph[0:100], pos_tagged_dict) # paragraph[0:100] to not dominate the dataframe with text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: for one story (expand this for multiple stories)\n",
    "    total_words = sum(len(pos_tagged_dict.get(tag, [])) for tag in allowed_pos_tags)  # Total words in the story\n",
    "    \n",
    "    # Calculate POS frequency ratios for the story\n",
    "    pos_frequencies = calculate_pos_frequencies(pos_tagged_dict, total_words)\n",
    "    \n",
    "    # Convert the POS frequencies to a DataFrame for easier comparison across stories\n",
    "    pos_freq_df5 = pd.DataFrame([pos_frequencies], index=['House_of_Usher'])  # 'Maelstrom' is the example story title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "6cc4949d-3b98-4d26-9df1-0ddbf09e7e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>RB</th>\n",
       "      <th>VB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>House_of_Usher</th>\n",
       "      <td>496.95987</td>\n",
       "      <td>262.667207</td>\n",
       "      <td>182.813133</td>\n",
       "      <td>57.559789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       NN          JJ          RB         VB\n",
       "House_of_Usher  496.95987  262.667207  182.813133  57.559789"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the POS frequency ratios (per 1000 words) for the story using Jupyter's built-in formatting for better viewability\n",
    "pos_freq_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "2f83a4e7-d359-4e41-ac55-c7b045fdebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the 5 POS frequency distributions into one dataframe\n",
    "pos_freq_df_5stories = pd.concat([pos_freq_df, pos_freq_df2, pos_freq_df3, pos_freq_df4, pos_freq_df5], ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "b180b046-9c77-4b67-bd26-4d6401bfe6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JJ</th>\n",
       "      <th>NN</th>\n",
       "      <th>RB</th>\n",
       "      <th>VB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Maelstrom</th>\n",
       "      <td>304.485155</td>\n",
       "      <td>599.494630</td>\n",
       "      <td>88.439672</td>\n",
       "      <td>7.580543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Berenice</th>\n",
       "      <td>356.635071</td>\n",
       "      <td>541.469194</td>\n",
       "      <td>91.232227</td>\n",
       "      <td>10.663507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amontillado</th>\n",
       "      <td>349.757224</td>\n",
       "      <td>558.544934</td>\n",
       "      <td>85.250338</td>\n",
       "      <td>6.447505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Black_Cat</th>\n",
       "      <td>328.947368</td>\n",
       "      <td>585.020243</td>\n",
       "      <td>77.935223</td>\n",
       "      <td>8.097166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>House_of_Usher</th>\n",
       "      <td>262.667207</td>\n",
       "      <td>496.959870</td>\n",
       "      <td>182.813133</td>\n",
       "      <td>57.559789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        JJ          NN          RB         VB\n",
       "Maelstrom       304.485155  599.494630   88.439672   7.580543\n",
       "Berenice        356.635071  541.469194   91.232227  10.663507\n",
       "Amontillado     349.757224  558.544934   85.250338   6.447505\n",
       "Black_Cat       328.947368  585.020243   77.935223   8.097166\n",
       "House_of_Usher  262.667207  496.959870  182.813133  57.559789"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the POS frequency ratios (per 1000 words) for all 5 stories using Jupyter's built-in formatting for better viewability\n",
    "pos_freq_df_5stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7408f1-7ddb-4551-acac-0e9d2a4b7210",
   "metadata": {},
   "source": [
    "To assess the distribution of parts of speech across these five stories, I will calculate the coefficient of variation among the five stories for each POS category. The coefficient of variation is a normalized measure of dispersion that is calculated by dividing the standard deviation of the data by the mean of the data. A small CV would indicate that the POS frequency ratio is consistent, supporting the conjecture that the number of different parts of speech per thousand words is pretty much the same for all stories in a given language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "868ee711-09a7-43f9-8156-93c281e97657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ    0.119161\n",
      "NN    0.072111\n",
      "RB    0.415724\n",
      "VB    1.224677\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean for each column\n",
    "mean = pos_freq_df_5stories.mean()\n",
    "\n",
    "# Calculate the standard deviation for each column\n",
    "std_dev = pos_freq_df_5stories.std()\n",
    "\n",
    "# Calculate the coefficient of variation (CV) for each column\n",
    "# Avoid division by zero (set CV to NaN where the mean is 0)\n",
    "cv = std_dev / mean.replace(0, pd.NA)\n",
    "\n",
    "# Display the coefficient of variation for each column\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c36537-300e-4956-8b4f-78acdd5177c9",
   "metadata": {},
   "source": [
    "Interpretation of CV findings:\n",
    "\n",
    "Small CV (< 10%): Indicates that the data is tightly clustered around the mean, reflecting consistency or low relative variability.\n",
    "\n",
    "Moderate CV (10-20%): Indicates some variability, but still suggests that the data points are relatively stable with moderate differences across samples.\n",
    "\n",
    "Large CV (> 20%): Indicates significant variability, meaning the data points are widely spread out relative to the mean. This could suggest inconsistency across the data.\n",
    "\n",
    "My findings suggest that adjectives (JJ; CV of 0.119 or 11.9%) and nouns (NN; CV of 0.072 or 7.2%) are highly consistent, with low relative variability between stories. This suggests that the proportion of adjectives and nouns relative to the total number of words in the stories is fairly consistent across stories.\n",
    "\n",
    "However, adverbs (RB; CV of 0.416 or 41.6%) and verbs (VB; CV of 1.225 or 122.5%) are highly variable between stories, suggesting that the proportion of adverbs and verbs relative to the total number of words in the stories is fairly inconsistent across stories.\n",
    "\n",
    "It is likely that if I conducted this analysis across all Poe stories the CV would look different for these parts of speech. However, with the analysis I have conducted with the processing power I have, the conjecture that the number of different parts of speech per thousand words is pretty much the same for all stories in a given language is consistent with my findings for adjectives and nouns, but not for adverbs and verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05844b2-aa76-4a26-ad66-6c616cab5505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
