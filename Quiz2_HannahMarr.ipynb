{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422f5658-ba42-4986-b730-fbb0bb3ecc79",
   "metadata": {},
   "source": [
    "# Quiz 02: Working with large files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24550c-8001-40c3-b6c6-c47ba5a3bdab",
   "metadata": {},
   "source": [
    "Primary author: Hannah Marr\n",
    "Collaborator: Emma Virnelli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645c0f1-34dc-44c9-b8bc-475d3646e641",
   "metadata": {},
   "source": [
    "## Linux Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49841d43-df0b-4f10-9b05-ea937adcafaa",
   "metadata": {},
   "source": [
    "\n",
    "Most coding assignments for big data require students to run programs on the EECS server or Google Cloud Platform web instance, which both are linux-based servers. To prepare you for the incoming coding assignments/quizzes, we encourage you to learn about these frequently used linux commands (man, ssh, ls, cd, pwd, mkdir, rm, cat, mv, cp, wc). To find usage of each command, you’re encouraged to use Google or the man command in a linux shell (Use Terminal app - MacOS OR login to Halligan/EECS Server). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107227b-2f41-4bbe-915c-6494af09a5c0",
   "metadata": {},
   "source": [
    "1. Answer the following questions in one or two sentences each:\n",
    "a) [½ point] What does the man pwd command tell you about the pwd command? You should learn what the man command is if you tried with different options, such as man ls, man cd. \n",
    "\n",
    "The man command in Linux displays the user manual for any command that can be run in the terminal. This includes information about the command's name, description, options, examples, and more.\n",
    "The man command tells me that “The pwd utility writes the absolute pathname of the current working directory to the standard output.” It gives me the Name, Synopsis, Description, Environment, Exit Status, Examples, See Also, Standards, History and Bugs sections of the manual (man), with relevant details in each section.\n",
    "Man provides details about accessing list directory contents, and man cd provides information about cd as well as other built-in commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4499a92-2e53-422a-b8c9-117942c47463",
   "metadata": {},
   "source": [
    "1. b) [1½ point] Explain piping in Linux.\n",
    "\n",
    "Piping in Linux is used to combine two or more commands, using the pipe character ‘|’. When doing this, the output of one command acts as input to another command. This command’s output may act as input to the next command, and so on. Piping allows these commands to operate simultaneously and allows data to be transferred continuously, rather than being passed through temporary files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570a86ca-81ae-4965-b587-ecdd45332678",
   "metadata": {},
   "source": [
    "1. c) [3 points] Explain the concepts of stdin, stdout and stderr.\n",
    "\n",
    "Stdin refers to the path a program uses to receive input, which usually comes from a keyboard or a file. This is used when a user inputs data into a program.\n",
    "Stdout is the path a program uses to output data, and is usually displayed where the program is running, on the console or terminal. This is used when a program prints information using stdout.\n",
    "Stderr outputs error messages and diagnostics. It is on a different path than stdout so that normal program output isn’t mixed with error messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a0a65-efaf-48fb-b549-62d95c59f13b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ae3d9-07bb-413b-9f35-39597fbef7de",
   "metadata": {},
   "source": [
    "## Handling Compressed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7e2e7-c90a-49fb-8fed-796355821080",
   "metadata": {},
   "source": [
    "For this part of the quiz, you may use your own laptop. On a Mac or a Chromebook, use Terminal. On Windows laptops, we recommend using the ubuntu. If you don’t have enough space on your laptop, you may use the eecs Linux servers. The eecs Linux servers already have the compressed file available as /comp/119/arcos_all_washpost.tsv.gz.\n",
    "\n",
    "2. [7½ points] Find the column names in the Opioid dataset. The naive way is to gunzip the .gz file and run head -1 on the result, but you likely don’t have enough disk space. Conveniently, zcat can read the file and write the unzipped contents into stdout, which can be piped into head -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13321b68-3023-4547-aa04-7521ddcbd00c",
   "metadata": {},
   "source": [
    "The following code was run in my Mac Terminal using Linux. I provide comments and reasoning below along with the code.\n",
    "\n",
    "(base) hannahmarr@sslvpn65 ~ % ssh hmarr01@linux.eecs.tufts.edu\n",
    "hmarr01@linux.eecs.tufts.edu's password: \n",
    "Disk quotas for user hmarr01 (uid 48215): \n",
    "     Filesystem   space   quota   limit   grace   files   quota   limit   grace\n",
    "vs-home:/tufts_grad\n",
    "                    28K   2765M   3072M              12    270k    300k        \n",
    "vm-linux01{hmarr01}1: zcat /comp/119/arcos_all_washpost.tsv.gz | head -1\n",
    "REPORTER_DEA_NO\tREPORTER_BUS_ACT\tREPORTER_NAME\tREPORTER_ADDL_CO_INFO\tREPORTER_ADDRESS1\tREPORTER_ADDRESS2\tREPORTER_CITY\tREPORTER_STATE\tREPORTER_ZIP\tREPORTER_COUNTY\tBUYER_DEA_NO\tBUYER_BUS_ACT\tBUYER_NAME\tBUYER_ADDL_CO_INFO\tBUYER_ADDRESS1\tBUYER_ADDRESS2\tBUYER_CITY\tBUYER_STATE\tBUYER_ZIP\tBUYER_COUNTY\tTRANSACTION_CODE\tDRUG_CODE\tNDC_NO\tDRUG_NAME\tQUANTITY\tUNIT\tACTION_INDICATOR\tORDER_FORM_NO\tCORRECTION_NO\tSTRENGTH\tTRANSACTION_DATE\tCALC_BASE_WT_IN_GM\tDOSAGE_UNIT\tTRANSACTION_ID\tProduct_Name\tIngredient_Name\tMeasureMME_Conversion_Factor\tCombined_Labeler_Name\tRevised_Company_Name\tReporter_family\tdos_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af7461-6e6d-48a0-b611-5ff9c87cfd9f",
   "metadata": {},
   "source": [
    "Code utilized (Input):\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | head -1\n",
    "\n",
    "Output:\n",
    "REPORTER_DEA_NO\tREPORTER_BUS_ACT\tREPORTER_NAME\tREPORTER_ADDL_CO_INFO\tREPORTER_ADDRESS1\tREPORTER_ADDRESS2\tREPORTER_CITY\tREPORTER_STATE\tREPORTER_ZIP\tREPORTER_COUNTY\tBUYER_DEA_NO\tBUYER_BUS_ACT\tBUYER_NAME\tBUYER_ADDL_CO_INFO\tBUYER_ADDRESS1\tBUYER_ADDRESS2\tBUYER_CITY\tBUYER_STATE\tBUYER_ZIP\tBUYER_COUNTY\tTRANSACTION_CODE\tDRUG_CODE\tNDC_NO\tDRUG_NAME\tQUANTITY\tUNIT\tACTION_INDICATOR\tORDER_FORM_NO\tCORRECTION_NO\tSTRENGTH\tTRANSACTION_DATE\tCALC_BASE_WT_IN_GM\tDOSAGE_UNIT\tTRANSACTION_ID\tProduct_Name\tIngredient_Name\tMeasureMME_Conversion_Factor\tCombined_Labeler_Name\tRevised_Company_Name\tReporter_family\tdos_str\n",
    "\n",
    "Explanation:\n",
    "The command zcat reads the compressed file /comp/119/arcos_all_washpost.tsv.gz and decompresses it. It then outputs the contents to stdout, which is read in my terminal.\n",
    "| is a pipe, which is used in Linux to 'pipe' the output of zcat into the next command (in this case, head -1).\n",
    "head -1 takes the decompressed file output from zcat and displays the headers (first line only) of the contents of the decompressed data sheet.\n",
    "Stdout then shows the headers of each data column within this dataset (shown above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99662a5f-4cf3-4ca7-8050-148fd413eb0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79974b9d-6c29-4462-9e3b-4e3fa1c54186",
   "metadata": {},
   "source": [
    "3. [7½ points] Find the number of rows in the Opioid dataset by processing the zcat output, stripping the header row, and counting the remaining lines using wc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5efeec-8337-4f86-aec7-c767cb7f1f8d",
   "metadata": {},
   "source": [
    "The following code was run in my Mac Terminal using Linux. I provide comments and reasoning below along with the code.\n",
    "\n",
    "Last login: Wed Sep 11 22:01:47 on ttys000\n",
    "(base) hannahmarr@Hannahs-MacBook-Pro ~ % ssh hmarr01@linux.eecs.tufts.edu\n",
    "hmarr01@linux.eecs.tufts.edu's password: \n",
    "Last login: Thu Sep 12 10:47:22 2024 from 10.243.29.69\n",
    "Disk quotas for user hmarr01 (uid 48215): \n",
    "     Filesystem   space   quota   limit   grace   files   quota   limit   grace\n",
    "vs-home:/tufts_grad\n",
    "                    32K   2765M   3072M              14    270k    300k        \n",
    "vm-linux01{hmarr01}1: zcat /comp/119/arcos_all_washpost.tsv.gz | tail -n +2 | wc -l\n",
    "178598026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea3ab5-940c-4fde-9668-49919341ade1",
   "metadata": {},
   "source": [
    "Code utilized (Input):\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | tail -n +2 | wc -l\n",
    "\n",
    "Output:\n",
    "178598026\n",
    "\n",
    "Explanation:\n",
    "The zcat command reads the compressed file /comp/119/arcos_all_washpost.tsv.gz and decompresses it. It then outputs the contents to stdout, which is read in my terminal.\n",
    "| is a pipe, which is used in Linux to 'pipe' the output of zcat into the next command (in this case, tail -n +2, which then pipes to wc -l).\n",
    "tail -n +2 skips the first line, since we are assuming that is the header row, and provides the rest of the lines in stdout. -n +2 means to start the stdout output from the second line, excluding the header.\n",
    "wc -l provides the count of the number of lines from the tail command's computation, which gives you the total number of rows in the dataset, excluding the header row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c504bd-4a54-492b-adc5-ccfc4e6567b8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e1a52-52f2-4ed8-90f0-bfa87e7667e3",
   "metadata": {},
   "source": [
    "4. [10 points] Find the names of all the drugs named in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a040ab6c-64c4-45f7-a0d9-e30d789f4176",
   "metadata": {},
   "source": [
    "Code utilized (Input):\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | awk -F'\\t' 'NR > 1 {print $24}' | sort -T /h/hmarr01 | uniq\n",
    "\n",
    "Output:\n",
    "HYDROCODONE\n",
    "OXYCODONE\n",
    "\n",
    "Explanation:\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz decompresses the file /comp/119/arcos_all_washpost.tsv.gz (compressed in the .gz format) and outputs the file contents to stdout, which is read in my terminal.\n",
    "| is a pipe, which is used in Linux to 'pipe' the output of zcat into the next command.\n",
    "awk is used for pattern scanning and data extraction, specifically for text processing. It is used here to extract data from specific columns. -F'\\t' is telling awk that the fields in the data file are separated by tabs (\\t).\n",
    "NR > 1 tells awk to skip the first row (we know in this file that the first row is made up of headers) and {print $24} is to print the 24th column, which we know is DRUG_NAME.\n",
    "sort -T h/hmarr01 is specifying the directory /h/hmarr01 as a temporary file storage location for the files used by the sort command, since we found that a data file of this size cannot be stored within the course servers housed in Halligan.\n",
    "uniq removes any duplicate lines from the sorted output, so that we see only the unique drug names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cfe0b2-7471-4a7b-9258-707591c2baab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f0e92-a22a-4750-ba43-c769ad0cf41c",
   "metadata": {},
   "source": [
    "# Analysis of the Opioid Dataset Using Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ea969-de8d-4798-9106-d5dbe67ade2e",
   "metadata": {},
   "source": [
    "5. [10 points] Estimate the number of rows for each year in the dataset. There may be enough space in the shell, but we’d prefer not to rely on that. So here’s a potential strategy: Use the shuf command to extract, say, random 7,500 rows from the output of zcat. Find the proportion of rows for each year in this extract. Assuming that the distribution of the random 7,500 rows is similar to the distribution in the whole file, estimate the number of rows for each year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bcc9f-0051-47e8-b61a-01873822b635",
   "metadata": {},
   "source": [
    "The following code was run in my Mac Terminal using Linux. I provide comments and reasoning below along with the code.\n",
    "\n",
    "vm-linux01{hmarr01}27: zcat /comp/119/arcos_all_washpost.tsv.gz | shuf -n 7500 > random_sample.tsv \n",
    "vm-linux01{hmarr01}28: awk -F \\t '{print$31}' random_sample.tsv > years.tsv grep -c '2006'\n",
    "vm-linux01{hmarr01}29: awk -F \\t '{print$31}' random_sample.tsv > years.tsv\n",
    "vm-linux01{hmarr01}30: grep -c '2006'\n",
    "vm-linux01{hmarr01}31: awk -F '\\t' '$0 ~ /2006/ {count++} END {print count}' random_sample.tsv\n",
    "864\n",
    "vm-linux01{hmarr01}32: awk -F '\\t' '$0 ~ /2007/ {count++} END {print count}' random_sample.tsv\n",
    "967\n",
    "vm-linux01{hmarr01}33: awk -F '\\t' '$0 ~ /2008/ {count++} END {print count}' random_sample.tsv\n",
    "1108\n",
    "vm-linux01{hmarr01}34: awk -F '\\t' '$0 ~ /2009/ {count++} END {print count}' random_sample.tsv\n",
    "1039\n",
    "vm-linux01{hmarr01}35: awk -F '\\t' '$0 ~ /2010/ {count++} END {print count}' random_sample.tsv\n",
    "1273\n",
    "vm-linux01{hmarr01}36: awk -F '\\t' '$0 ~ /2011/ {count++} END {print count}' random_sample.tsv\n",
    "1252\n",
    "vm-linux01{hmarr01}37: awk -F '\\t' '$0 ~ /2012/ {count++} END {print count}' random_sample.tsv\n",
    "1298"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db92f6-0749-4668-9242-8b17d46a2997",
   "metadata": {},
   "source": [
    "Code utilized (Input): \n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | shuf -n 7500 > random_sample.tsv \n",
    "awk -F \\t '{print$31}' random_sample.tsv > years.tsv \n",
    "awk -F '\\t' '$0 ~ /2006/ {count++} END {print count}' random_sample.tsv\n",
    "Repeat for 2006 to 2012 (see above for full code)\n",
    "\n",
    "Output:\n",
    "See above for output from each individual line of code. Output summary is as follows:\n",
    "2006: 864\n",
    "2007: 967\n",
    "2008: 1108\n",
    "2009: 1039\n",
    "2010: 1273\n",
    "2011: 1252\n",
    "2012: 1298\n",
    "\n",
    "Explanation:\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | shuf -n 7500 > random_sample.tsv :\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz decompresses the file /comp/119/arcos_all_washpost.tsv.gz (compressed in the .gz format) and outputs the file contents to stdout, which is read in my terminal.\n",
    "| is a pipe, which is used in Linux to 'pipe' the output of zcat into the next command.\n",
    "shuf -n 7500 randomly shuffles all the rows in the file and randomly selects 7,500 rows.\n",
    "> random_sample.tsv redirects the randomly sampled 7,500 lines into a new file called random_sample.tsv.\n",
    "\n",
    "awk -F \\t '{print$31}' random_sample.tsv > years.tsv :\n",
    "awk is used for pattern scanning and data extraction, specifically for text processing. It is used here to extract data from specific columns. -F'\\t' is telling awk that the fields in the data file are separated by tabs (\\t).\n",
    "{print$31} prints data from the 31st column of each row.\n",
    "random_sample.tsv is the input file for this command, which contains the randomly selected 7,500 lines from the previous line of code. > years.tsv redirects the output from the 31st column to a new file called years.tsv.\n",
    "\n",
    "awk -F '\\t' '$0 ~ /2006/ {count++} END {print count}' random_sample.tsv (for years 2006-2012):\n",
    "As in the above code, awk is used for pattern scanning and data extraction, specifically for text processing. It is used here to extract data from specific columns. -F'\\t' is telling awk that the fields in the data file are separated by tabs (\\t).\n",
    "$0 ~ /2006/ searches for any lines containing the string '2006'.\n",
    "{count++} increments a counter, beginning at zero, that increases upon each encounter of a line containing the string '2006'.\n",
    "random_sample.tsv is the input file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412bf21-8338-48de-8a22-eaecdc2ffed5",
   "metadata": {},
   "source": [
    "Below is the table which I used to calculate the estimated number of rows per year across the entire dataset, assuming the same distribution of years as in the random sample of 7,500 lines.\n",
    "\n",
    "I used the logic that the proportion of rows per year in the random sample = n rows per year / 7500 (total rows in random sample). With the assumption that the random distribution of years is representative of the entire distribution of years across the entire dataset, it follows that n rows per year / 7500 = x / 178598026 (total # of rows in entire dataset, as found above). I can solve for x by multiplying 178598026 by the proportion n rows per year / 7500. These calculations are done below.\n",
    "To sanity check, the sum of each value in the 'estimated # rows/year across entire datset' column = 185741948\n",
    "We would expect this value to be slightly off from the true total number of rows in the dataset (178598026), since the distribution of years across the entire dataset is likely not exactly equivalent to the distribution in our random sample. This shows that our assumption that the random distribution of years from 7500 lines is representative of the entire dataset distribution of years is not quite true, but fairly close to the truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b1f16-b4bb-40c1-99fd-357d60fc7722",
   "metadata": {},
   "source": [
    "| Year | # of rows/year from random distribution | proportion rows/year | estimated # rows/year across entire dataset|\n",
    "| ---- | --------------------------------------- | -------------------- | ------------------------------------------ |\n",
    "| 2006 | 864                                     | 864/7500 = 0.1152    | 0.1152 * 178598026 = ~20574493             |\n",
    "| 2007 | 967                                     | 967/7500 = 0.1289    | 0.1289 * 178598026 = ~23021286             |\n",
    "| 2008 | 1108                                    | 1108/7500 = 0.1477   | 0.1477 * 178598026 = ~26378928             |\n",
    "| 2009 | 1039                                    | 1039/7500 = 0.1385   | 0.1385 * 178598026 = ~24735827             |\n",
    "| 2010 | 1273                                    | 1273/7500 = 0.1697   | 0.1697 * 178598026 = ~30308085             |\n",
    "| 2011 | 1252                                    | 1252/7500 = 0.1669   | 0.1669 * 178598026 = ~29808011             |\n",
    "| 2012 | 1298                                    | 1298/7500 = 0.1731   | 0.1731 * 178598026 = ~30915318             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92deb0b4-ae90-4209-b10d-6bbec0e9cc10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343bde47-2479-4b4c-b867-b095a396b1fd",
   "metadata": {},
   "source": [
    "6. [10 points] Extract any 4,000 rows for the year 2012 from arcos_all_washpost.tsv.gz. Write the resulting lines to arcos_2012.tsv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076aabd9-b1ac-4f7b-9892-6fd6bb469fba",
   "metadata": {},
   "source": [
    "The following code was run in my Mac Terminal using Linux. I provide comments and reasoning below along with the code.\n",
    "\n",
    "vm-linux01{hmarr01}39: zcat /comp/119/arcos_all_washpost.tsv.gz | awk -F'\\t' '{if (substr($31, 5, 4) == \"2012\")print $0}' | head -n 4000 > arcos_2012.tsv\n",
    "vm-linux01{hmarr01}40: head arcos_2012.tsv\n",
    "PA0006836\tDISTRIBUTOR\tACE SURGICAL SUPPLY CO INC\tnull\t1034 PEARL STREET\tnull\tBROCKTON\tMA\t2301\tPLYMOUTH\tBT3484653\tPRACTITIONER\tTABRIZI, HAMID R DMD\tnull\t389 MAIN STREET, SUITE 404\tnull\tMALDEN\tMA\t2148\tMIDDLESEX\tS\t9193\t00406036301\tHYDROCODONE\t1.0\tnull\tnull\tnull\tnull\tnull\t12262012\t0.6054\t100.0\t64\tHYDROCODONE BIT/ACETA 10MG/500MG USP\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tSpecGx LLC\tMallinckrodt\tACE Surgical Supply Co Inc\t10.0\n",
    "PA0021179\tDISTRIBUTOR\tAPOTHECA INC\tnull\t1622 N 16TH ST\tnull\tPHOENIX\tAZ\t85006\tMARICOPA\tBJ5858002\tPRACTITIONER\tJEFFERS, WILL VAHID DO\tNOW CARE\t8251 W UNION HILLS DR\tSUITE 140\tGLENDALE\tAZ\t85308\tMARICOPA\tS\t9193\t12634051485\tHYDROCODONE\t10.0\tnull\tnull\tnull\tnull\tnull\t12052012\t0.45405\t150.0\t5\tHYDROCODONE BITARTRATE & ACETA  5MG/\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tApotheca Inc.\tApotheca Inc.\tApotheca Inc\t5.0\n",
    "PA0021179\tDISTRIBUTOR\tAPOTHECA INC\tnull\t1622 N 16TH ST\tnull\tPHOENIX\tAZ\t85006\tMARICOPA\tBK7485863\tPRACTITIONER-DW/275\tKHOURY, MUFID MD\tnull\t6220 W BELL ROAD SUITE 100\tnull\tGLENDALE\tAZ\t85308\tMARICOPA\tS\t9193\t12634051480\tHYDROCODONE\t10.0\tnull\tnull\tnull\tnull\tnull\t07242012\t0.6054\t200.0\t64\tHYDROCODONE BITARTRATE & ACETA  5MG/\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tApotheca Inc.\tApotheca Inc.\tApotheca Inc\t5.0\n",
    "PA0021179\tDISTRIBUTOR\tAPOTHECA INC\tnull\t1622 N 16TH ST\tnull\tPHOENIX\tAZ\t85006\tMARICOPA\tBS7305039\tPRACTITIONER\tSAWYER, JEFFRY S MD\tnull\t5615 S SOSSAMAN RD\tnull\tMESA\tAZ\t85212\tMARICOPA\t9193\t12634078371\tHYDROCODONE\t10.0\tnull\tnull\tnull\tnull\tnull\t02042012\t1.36215\t300.0\t16\tHYDROCODONEBITARTRATE & ACETA  7.5MG\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tApotheca Inc.\tApotheca Inc.Apotheca Inc\t7.5\n",
    "PA0021179\tDISTRIBUTOR\tAPOTHECA INC\tnull\t1622 N 16TH ST\tnull\tPHOENIX\tAZ\t85006\tMARICOPA\tFH2969509\tPRACTITIONER\tHOBBS, DOUGLAS DON MD\tURGENT CARE EXTRA AZ\t6501 E. GREENWAY PARKWAY\tSUITE 3-104\tSCOTTSDALE\tAZ\t85254\tMARICOPA\tS\t9193\t12634078371\tHYDROCODONE\t20.0\tnull\tnull\tnull\tnull\tnull\t05292012\t2.7243\t600.0\t45\tHYDROCODONEBITARTRATE & ACETA  7.5MG\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tApotheca Inc.\tApotheca Inc.\tApotheca Inc\t7.5\n",
    "PA0021179\tDISTRIBUTOR\tAPOTHECA INC\tnull\t1622 N 16TH ST\tnull\tPHOENIX\tAZ\t85006\tMARICOPA\tFH3007413\tPRACTITIONER\tHOBBS, DOUGLAS D MD\tnull\t1641 E GUADALUPE RD\tnull\tGILBERT\tAZ\t85233\tMARICOPA\t9193\t12634051471\tHYDROCODONE\t20.0\tnull\tnull\tnull\tnull\tnull\t11012012\t1.8162\t600.0\t11\tHYDROCODONE BITARTRATE & ACETA  5MG/\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tApotheca Inc.\tApotheca Inc.Apotheca Inc\t5.0\n",
    "PA0021179\tDISTRIBUTOR\tAPOTHECA INC\tnull\t1622 N 16TH ST\tnull\tPHOENIX\tAZ\t85006\tMARICOPA\tFS0458744\tPRACTITIONER\tSAWYER, JEFFRY S\tnull\t3200 S. GILBERT RD\tnull\tCHANDLER\tAZ\t85286\tMARICOPA\tS\t9193\t12634051471\tHYDROCODONE\t20.0\tnull\tnull\tnull\tnull\tnull\t05262012\t1.816600.0\t38\tHYDROCODONE BITARTRATE & ACETA  5MG/\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tApotheca Inc.Apotheca Inc.\tApotheca Inc\t5.0\n",
    "PA0021179\tDISTRIBUTOR\tAPOTHECA INC\tnull\t1622 N 16TH ST\tnull\tPHOENIX\tAZ\t85006\tMARICOPA\tFS0458744\tPRACTITIONER\tSAWYER, JEFFRY S\tnull\t3200 S. GILBERT RD\tnull\tCHANDLER\tAZ\t85286\tMARICOPA\tS\t9193\t12634078371\tHYDROCODONE\t20.0\tnull\tnull\tnull\tnull\tnull\t08012012\t2.724600.0\t3\tHYDROCODONEBITARTRATE & ACETA  7.5MG\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tApotheca Inc.Apotheca Inc.\tApotheca Inc\t7.5\n",
    "PA0021179\tDISTRIBUTOR\tAPOTHECA INC\tnull\t1622 N 16TH ST\tnull\tPHOENIX\tAZ\t85006\tMARICOPA\tFS1390424\tPRACTITIONER\tSAWYER, JEFFRY S MD\tnull\t6323 S. RURAL RD\tSUITE 105-107\tTEMPE\tAZ\t85283\tMARICOPA\tS\t9193\t12634078371\tHYDROCODONE\t40.0\tnull\tnull\tnull\tnull\tnull\t02172012\t5.4481200.0\t48\tHYDROCODONEBITARTRATE & ACETA  7.5MG\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tApotheca Inc.Apotheca Inc.\tApotheca Inc\t7.5\n",
    "PA0021179\tDISTRIBUTOR\tAPOTHECA INC`b\tnull\t1622 N 16TH ST\tnull\tPHOENIX\tAZ\t85006\tMARICOPA\tFS2023911\tPRACTITIONER\tSAWYER, JEFFRY MD\tEAST VALLEY URGENT CARE, LLC\t1120 S. GILBERT RD\tnull\tMESA\tAZ\t85204\tMARICOPA\tS\t9193\t12634051471\tHYDROCODONE\t40.0\tnull\tnull\tnull\tnull\tnull\t07262012\t3.6324\t1200.0\t80\tHYDROCODONE BITARTRATE & ACETA  5MG/\tHYDROCODONE BITARTRATE HEMIPENTAHYDRATE\tTAB\t1.0\tApotheca Inc.\tApotheca Inc.\tApotheca Inc\t5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03e688-731f-4b23-b774-d71bb688358e",
   "metadata": {},
   "source": [
    "Code utilized (Input):\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | awk -F'\\t' '{if (substr($31, 5, 4) == \"2012\")print $0}' | head -n 4000 > arcos_2012.tsv\n",
    "head arcos_2012.tsv\n",
    "\n",
    "Output:\n",
    "See above for output from 'head arcos_2012.tsv'.\n",
    "\n",
    "Explanation:\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | awk -F'\\t' '{if (substr($31, 5, 4) == \"2012\")print $0}' | head -n 4000 > arcos_2012.tsv :\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz decompresses the file /comp/119/arcos_all_washpost.tsv.gz (compressed in the .gz format) and outputs the file contents to stdout, which is read in my terminal.\n",
    "| is a pipe, which is used in Linux to 'pipe' the output of zcat into the next command.\n",
    "awk is used for pattern scanning and data extraction, specifically for text processing. It is used here to extract data from specific columns. -F'\\t' is telling awk that the fields in the data file are separated by tabs (\\t).\n",
    "substr($31, 5, 4) is the action that awk is performing. It extracts a substring from the 31st column, starting at the 5th character and taking the next 4 characters to check if the substring equals '2012'. If this condition is found to be true, it prints the entire row.\n",
    "head -n 4000 outputs the first 4000 lines of matching data, and > arcos_2012.tsv stores that data in a file called arcos_2012.tsv.\n",
    "\n",
    "head arcos_2012.tsv :\n",
    "I use this code above to sanity check myself and view the first 10 lines of data stored in the arcos_2012.tsv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d22e96-49ca-4e55-a25f-69504a2b3943",
   "metadata": {},
   "source": [
    "I will use the following questions to prove that the data I stored in the arcos_2012.tsv file matches the assumptions I have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ebfc1-91a6-49a2-9264-28bc12df50c3",
   "metadata": {},
   "source": [
    "Does it have the same header as the original file?\n",
    "\n",
    "Yes, my file shows the same headers as the original file. Please see explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c3c73-1503-4976-b6d0-6b602caf3595",
   "metadata": {},
   "source": [
    "The following code was run in my Mac Terminal using Linux. I provide comments and reasoning below along with the code.\n",
    "\n",
    "vm-linux01{hmarr01}45: zcat /comp/119/arcos_all_washpost.tsv.gz | head -n 1 > arcos_2012_header.tsv\n",
    "vm-linux01{hmarr01}46: zcat /comp/119/arcos_all_washpost.tsv.gz | head -n 1\n",
    "REPORTER_DEA_NO\tREPORTER_BUS_ACT\tREPORTER_NAME\tREPORTER_ADDL_CO_INFO\tREPORTER_ADDRESS1\tREPORTER_ADDRESS2\tREPORTER_CITY\tREPORTER_STATE\tREPORTER_ZIP\tREPORTER_COUNTY\tBUYER_DEA_NO\tBUYER_BUS_ACT\tBUYER_NAME\tBUYER_ADDL_CO_INFO\tBUYER_ADDRESS1\tBUYER_ADDRESS2\tBUYER_CITY\tBUYER_STATE\tBUYER_ZIP\tBUYER_COUNTY\tTRANSACTION_CODE\tDRUG_CODE\tNDC_NO\tDRUG_NAME\tQUANTITY\tUNIT\tACTION_INDICATOR\tORDER_FORM_NO\tCORRECTION_NO\tSTRENGTH\tTRANSACTION_DATE\tCALC_BASE_WT_IN_GM\tDOSAGE_UNIT\tTRANSACTION_ID\tProduct_Name\tIngredient_Name\tMeasure\tMME_Conversion_Factor\tCombined_Labeler_Name\tRevised_Company_Name\tReporter_family\tdos_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a94a2c-b44c-4d68-a48b-dfbbd0ee948f",
   "metadata": {},
   "source": [
    "Code utilized (Input):\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | head -n 1 > arcos_2012_header.tsv\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | head -n 1\n",
    "\n",
    "Output:\n",
    "REPORTER_DEA_NO\tREPORTER_BUS_ACT\tREPORTER_NAME\tREPORTER_ADDL_CO_INFO\tREPORTER_ADDRESS1\tREPORTER_ADDRESS2\tREPORTER_CITY\tREPORTER_STATE\tREPORTER_ZIP\tREPORTER_COUNTY\tBUYER_DEA_NO\tBUYER_BUS_ACT\tBUYER_NAME\tBUYER_ADDL_CO_INFO\tBUYER_ADDRESS1\tBUYER_ADDRESS2\tBUYER_CITY\tBUYER_STATE\tBUYER_ZIP\tBUYER_COUNTY\tTRANSACTION_CODE\tDRUG_CODE\tNDC_NO\tDRUG_NAME\tQUANTITY\tUNIT\tACTION_INDICATOR\tORDER_FORM_NO\tCORRECTION_NO\tSTRENGTH\tTRANSACTION_DATE\tCALC_BASE_WT_IN_GM\tDOSAGE_UNIT\tTRANSACTION_ID\tProduct_Name\tIngredient_Name\tMeasure\tMME_Conversion_Factor\tCombined_Labeler_Name\tRevised_Company_Name\tReporter_family\tdos_str\n",
    "\n",
    "Explanation:\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | head -n 1 > arcos_2012_header.tsv :\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz decompresses the file /comp/119/arcos_all_washpost.tsv.gz (compressed in the .gz format) and outputs the file contents to stdout, which is read in my terminal.\n",
    "| is a pipe, which is used in Linux to 'pipe' the output of zcat into the next command (in this case, head).\n",
    "head outputs the first 10 lines of a file; the -n 1 addition prints only the first line (the header row in this case).\n",
    "> arcos_2012_header.tsv redirects the output of head into a new file called arcos_2012_header.tsv.\n",
    "\n",
    "zcat /comp/119/arcos_all_washpost.tsv.gz | head -n 1 :\n",
    "This line of code does the same thing as the above line; however, rather than storing the output of head in a new file, it is displayed in stdout, where I will see the header names of my file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16490b3f-bae6-406b-97e1-ab7c4a5bc681",
   "metadata": {},
   "source": [
    "Does it have exactly 4000 rows?\n",
    "\n",
    "Yes, my file has exactly 4000 rows. Please see explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc7480-554f-402a-a071-031637cfa038",
   "metadata": {},
   "source": [
    "The following code was run in my Mac Terminal using Linux. I provide comments and reasoning below along with the code.\n",
    "\n",
    "vm-linux01{hmarr01}47:  wc -l arcos_2012.tsv\n",
    "4000 arcos_2012.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62e5fb3-14a8-4d1d-8aa9-bdd04d8a9c35",
   "metadata": {},
   "source": [
    "Code utilized (Input):\n",
    "wc -l arcos_2012.tsv\n",
    "\n",
    "Output:\n",
    "4000 arcos_2012.tsv\n",
    "\n",
    "Explanation:\n",
    "wc stands for 'word count', but it also counts lines, characters, words, etc. in a file, depending on what is specified. In this case, -l tells wc to count the number of lines in the file.\n",
    "arcos_2012.tsv is the file I created, in which I am counting the number of lines.\n",
    "This command outputs the number of lines in the file arcos_2012.tsv, which is, in this case, 4000 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e0200-ab9e-4db7-b40c-576589ce37d3",
   "metadata": {},
   "source": [
    "Are all 4000 rows from the year 2012?\n",
    "\n",
    "Yes, all 4000 rows in my file are from the year 2012. Please see explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a26a1-fa03-40a4-b91a-a88ae7ba5692",
   "metadata": {},
   "source": [
    "The following code was run in my Mac Terminal using Linux. I provide comments and reasoning below along with the code.\n",
    "\n",
    "vm-linux01{hmarr01}48: awk -F'\\t' '{if (substr($31, 5, 4) != \"2012\")print $0}' arcos_2012.tsv\n",
    "# No output is returned, that means all rows have \"2012\" in column 31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d9510-9556-4bb6-89ff-b640b14d7002",
   "metadata": {},
   "source": [
    "Code utilized (Input): awk -F'\\t' '{if (substr($31, 5, 4) != \"2012\")print $0}' arcos_2012.tsv\n",
    "\n",
    "Output:\n",
    "None\n",
    "\n",
    "Explanation:\n",
    "awk is used for pattern scanning and data extraction, specifically for text processing. It is used here to extract data from specific columns. -F'\\t' is telling awk that the fields in the data file are separated by tabs (\\t).\n",
    "substr($31, 5, 4) extracts a substring from the 31st column of the file, starting at the 5th character with a length of 4 characters. != \"2012\" checks if the substring extracted is not equal to '2012'. print $0 prints the entire line if the extracted substring condition is true (in this case, that the substring does not equal 2012).\n",
    "Since there is no output from this command, there are no lines in this file in which the substring from the 5th to the 8th character in the 31st column does not equal 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb435e4-55a4-406f-8614-6708408edc37",
   "metadata": {},
   "source": [
    "Are there any duplicates among the 4000 rows?\n",
    "\n",
    "No, there are no duplicates in the 4000 rows of my file. Please see explanation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef6011-f42c-4c11-b4fe-b9fd613905a8",
   "metadata": {},
   "source": [
    "The following code was run in my Mac Terminal using Linux. I provide comments and reasoning below along with the code.\n",
    "\n",
    "vm-linux01{hmarr01}49: sort arcos_2012.tsv | uniq -d\n",
    "# If the output is empty, there are no duplicates. Any rows printed are duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229181e-0447-4187-821c-e901fc07ef1a",
   "metadata": {},
   "source": [
    "Code utilized (Input): sort arcos_2012.tsv | uniq -d\n",
    "\n",
    "Output:\n",
    "None\n",
    "\n",
    "Explanation:\n",
    "The sort command sorts the file arcos_2012.tsv in alphabetical and/or numerical order.\n",
    "uniq -d finds duplicate lines (-d is a flag that stands for duplicates) and prints them.\n",
    "Since there is no output from this command, there are no duplicate lines in this file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
